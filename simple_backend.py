from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uuid
import random
import pickle
import os
from datetime import datetime
import numpy as np
from scipy.stats import norm
from groq import Groq
import json
import asyncio
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

app = FastAPI()

# CORS middleware - Production ready configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000", 
        "http://localhost:3002", 
        "http://localhost:3005",
        "https://*.vercel.app",  # Allow all Vercel deployments
        "https://your-frontend-domain.vercel.app",  # Replace with your specific domain
        # Remove "*" for production security
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# Initialize Groq client
# GROQ_API_KEY ÙÙˆØ§Ø¦Ø¯ Ù…ÙØªØ§Ø­ 
# 1. ÙŠÙˆÙØ± Ø£Ø³Ø¦Ù„Ø© Ø£ÙƒØ«Ø± Ø°ÙƒØ§Ø¡Ù‹ ÙˆØªØ®ØµØµØ§Ù‹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
# 2. ÙŠØ­Ø³Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ÙØ³ÙŠ Ù…Ù† Ø®Ù„Ø§Ù„ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©  
# 3. ÙŠØªÙŠØ­ ØªØ®ØµÙŠØµ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø­Ø³Ø¨ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¯ÙŠÙ…ÙˆØºØ±Ø§ÙÙŠØ©
# 4. ÙŠØ¶Ù…Ù† ØªÙ†ÙˆØ¹ Ø£ÙƒØ¨Ø± ÙÙŠ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ¹Ø¯Ù… Ø§Ù„ØªÙƒØ±Ø§Ø±
# 5. ÙŠÙˆÙØ± ØªØ­Ù„ÙŠÙ„ Ø£Ø¹Ù…Ù‚ ÙˆØ£ÙƒØ«Ø± Ø¯Ù‚Ø© Ù„Ù„Ø´Ø®ØµÙŠØ©
GROQ_API_KEY = os.getenv("GROQ_API_KEY")  # Remove hardcoded key for security
groq_client = None

try:
    if GROQ_API_KEY:
        from groq import Groq
        groq_client = Groq(api_key=GROQ_API_KEY)
        print("Groq client initialized successfully")
    else:
        print("GROQ_API_KEY not found, using fallback question generation")
except ImportError:
    print("Groq package not installed, using fallback question generation")
    groq_client = None
except Exception as e:
    print(f"Error initializing Groq client: {e}")
    groq_client = None

# File-based storage
DATA_DIR = "/app/data" if os.getenv("HF_SPACE") else "./data"
os.makedirs(DATA_DIR, exist_ok=True)
SESSIONS_FILE = os.path.join(DATA_DIR, "sessions_data.pkl")
QUESTIONS_CACHE_FILE = os.path.join(DATA_DIR, "generated_questions_cache.pkl")

# Big Five personality dimensions
PERSONALITY_DIMENSIONS = [
    "extraversion",     # Ø§Ù„Ø§Ù†Ø¨Ø³Ø§Ø·
    "agreeableness",    # Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„ÙŠØ©  
    "conscientiousness", # Ø§Ù„Ø¶Ù…ÙŠØ±
    "neuroticism",      # Ø§Ù„Ø¹ØµØ§Ø¨ÙŠØ©
    "openness"          # Ø§Ù„Ø§Ù†ÙØªØ§Ø­ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø±Ø¨Ø©
]

# IRT and CAT Parameters
class IRTParameters:
    def __init__(self):
        self.max_questions = 50   # Total maximum questions (reduced from 200 to 50)
        self.min_questions = 25   # Minimum questions total (5 per dimension)
        self.max_per_dimension = 10  # Maximum questions per dimension (reduced from 40 to 10)
        self.min_per_dimension = 5   # Minimum questions per dimension (reduced from 16 to 5)
        self.target_se = 0.3     # Target standard error for stopping
        self.min_theta = -3.0    # Minimum ability level
        self.max_theta = 3.0     # Maximum ability level
        
# Advanced Question Generation with Groq AI
class QuestionGenerator:
    def __init__(self, groq_client):
        self.client = groq_client
        self.cache = self.load_cache()
    
    def load_cache(self):
        """Load cached questions from file"""
        if os.path.exists(QUESTIONS_CACHE_FILE):
            try:
                with open(QUESTIONS_CACHE_FILE, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                print(f"Error loading questions cache: {e}")
        return {}
    
    def save_cache(self):
        """Save cached questions to file"""
        try:
            with open(QUESTIONS_CACHE_FILE, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            print(f"Error saving questions cache: {e}")
    
    async def generate_personalized_questions(self, demographics: Dict, dimension: str, count: int = 10):
        """Generate personalized questions using optimized approach"""
        
        # Create cache key based on demographics and dimension
        cache_key = f"{dimension}_{demographics.get('age_group', 'unknown')}_{demographics.get('gender', 'unknown')}_{demographics.get('education_level', 'unknown')}_{demographics.get('marital_status', 'unknown')}"
        
        # Check cache first (instant return)
        if cache_key in self.cache:
            print(f"âœ… Using cached questions for {cache_key}")
            return self.cache[cache_key]
        
        # Use pre-built question bank for immediate response
        print(f"ðŸš€ Using optimized question bank for {dimension}")
        questions = self._get_optimized_question_bank(dimension, demographics, count)
        
        # Cache for future use
        self.cache[cache_key] = questions
        self.save_cache()
        
        # Generate AI questions in background (non-blocking)
        if self.client:
            asyncio.create_task(self._generate_ai_questions_background(demographics, dimension, cache_key))
        
        return questions
        
        # Define dimension descriptions in Arabic
        dimension_descriptions = {
            "extraversion": "Ø§Ù„Ø§Ù†Ø¨Ø³Ø§Ø· ÙˆØ§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ",
            "agreeableness": "Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„ÙŠØ© ÙˆØ§Ù„ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†", 
            "conscientiousness": "Ø§Ù„Ø¶Ù…ÙŠØ± ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ… ÙˆØ§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ©",
            "neuroticism": "Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ ÙˆØ§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±",
            "openness": "Ø§Ù„Ø§Ù†ÙØªØ§Ø­ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ù„Ø¥Ø¨Ø¯Ø§Ø¹"
        }
        
        # Create demographic context
        age_context = self._get_age_context(demographics.get('birth_year'))
        education_context = self._get_education_context(demographics.get('education_level'))
        marital_context = self._get_marital_context(demographics.get('marital_status'))
        gender_context = demographics.get('gender', 'unknown')
        
        prompt = f"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø¹Ù„Ù… Ø§Ù„Ù†ÙØ³ ÙˆØªØ·ÙˆÙŠØ± Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ©. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ {count} Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ Ø¨ÙØ¹Ø¯ {dimension_descriptions[dimension]}.

Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¯ÙŠÙ…ÙˆØºØ±Ø§ÙÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…:
- Ø§Ù„Ø¹Ù…Ø±: {age_context}
- Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ: {education_context}  
- Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©: {marital_context}
- Ø§Ù„Ø¬Ù†Ø³: {gender_context}

Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:
1. ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¯ÙŠÙ…ÙˆØºØ±Ø§ÙÙŠ Ø§Ù„Ù…Ø­Ø¯Ø¯
2. Ø§Ø³ØªØ®Ø¯Ù… ØµÙŠØºØ© "Ù‡Ù„ Ø£Ù†Øª/Ø£Ù†ØªÙ..." Ø£Ùˆ "Ø¥Ù„Ù‰ Ø£ÙŠ Ù…Ø¯Ù‰ ØªÙˆØ§ÙÙ‚/ØªÙˆØ§ÙÙ‚ÙŠÙ†..."
3. ØªÙ†ÙˆØ¹ ÙÙŠ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµØ¹ÙˆØ¨Ø© (Ø³Ù‡Ù„ØŒ Ù…ØªÙˆØ³Ø·ØŒ ØµØ¹Ø¨)
4. ØªØªØ¨Ø¹ Ù†Ø¸Ø±ÙŠØ© Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù…ÙØ±Ø¯Ø© (IRT)
5. ØªÙ‚ÙŠØ³ Ø¬ÙˆØ§Ù†Ø¨ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ø¨ÙØ¹Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨

Ø£Ø±Ø¬Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙƒÙ€ JSON array Ø¨Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„ØªØ§Ù„ÙŠ:
[
  {{
    "text": "Ù†Øµ Ø§Ù„Ø³Ø¤Ø§Ù„",
    "difficulty": -2.0 Ø¥Ù„Ù‰ 2.0,
    "discrimination": 0.5 Ø¥Ù„Ù‰ 2.5,
    "reverse_scored": true/false,
    "subdimension": "Ø§Ù„Ø¬Ø§Ù†Ø¨ Ø§Ù„ÙØ±Ø¹ÙŠ"
  }}
]
"""

        try:
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model="llama3-8b-8192",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=4000
            )
            
            # Parse the response
            content = response.choices[0].message.content
            
            # Extract JSON from response
            start_idx = content.find('[')
            end_idx = content.rfind(']') + 1
            
            if start_idx != -1 and end_idx > start_idx:
                json_str = content[start_idx:end_idx]
                questions_data = json.loads(json_str)
                
                # Process and add IDs
                questions = []
                for i, q in enumerate(questions_data):
                    question = {
                        "question_id": f"{dimension}_{cache_key}_{i+1}",
                        "text": q.get("text", ""),
                        "difficulty": float(q.get("difficulty", 0.0)),
                        "discrimination": float(q.get("discrimination", 1.0)),
                        "reverse_scored": q.get("reverse_scored", False),
                        "subdimension": q.get("subdimension", ""),
                        "dimension": dimension,
                        "demographic_context": demographics
                    }
                    questions.append(question)
                
                # Cache the questions
                self.cache[cache_key] = questions
                self.save_cache()
                
                print(f"Generated {len(questions)} questions for {dimension} - {cache_key}")
                return questions
                
        except Exception as e:
            print(f"Error generating questions with Groq: {e}")
            
        # Fallback to default questions if Groq fails
        return self._get_optimized_question_bank(dimension, demographics, count)
    
    def _get_age_context(self, birth_year):
        if not birth_year:
            return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
        
        try:
            age = 2025 - int(birth_year)
            if age < 20:
                return f"Ø´Ø§Ø¨/Ø´Ø§Ø¨Ø© ({age} Ø³Ù†Ø©)"
            elif age < 30:
                return f"ÙÙŠ Ø§Ù„Ø¹Ø´Ø±ÙŠÙ†Ø§Øª ({age} Ø³Ù†Ø©)"
            elif age < 50:
                return f"Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¹Ù…Ø± ({age} Ø³Ù†Ø©)"
            else:
                return f"ÙƒØ¨ÙŠØ± Ø§Ù„Ø³Ù† ({age} Ø³Ù†Ø©)"
        except:
            return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
    
    def _get_education_context(self, education_level):
        education_map = {
            "high_school": "Ø«Ø§Ù†ÙˆÙŠØ© Ø¹Ø§Ù…Ø©",
            "diploma": "Ø¯Ø¨Ù„ÙˆÙ…",
            "bachelor": "Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³", 
            "master": "Ù…Ø§Ø¬Ø³ØªÙŠØ±",
            "phd": "Ø¯ÙƒØªÙˆØ±Ø§Ù‡"
        }
        return education_map.get(education_level, "ØºÙŠØ± Ù…Ø­Ø¯Ø¯")
    
    def _get_marital_context(self, marital_status):
        marital_map = {
            "single": "Ø£Ø¹Ø²Ø¨/Ø¹Ø²Ø¨Ø§Ø¡",
            "married": "Ù…ØªØ²ÙˆØ¬/Ù…ØªØ²ÙˆØ¬Ø©",
            "divorced": "Ù…Ø·Ù„Ù‚/Ù…Ø·Ù„Ù‚Ø©",
            "widowed": "Ø£Ø±Ù…Ù„/Ø£Ø±Ù…Ù„Ø©"
        }
        return marital_map.get(marital_status, "ØºÙŠØ± Ù…Ø­Ø¯Ø¯")
    
    def _get_optimized_question_bank(self, dimension, demographics, count):
        """Get high-quality questions instantly from pre-built bank"""
        import random
        
        # Comprehensive question bank with proper IRT parameters
        question_banks = {
            "extraversion": [
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„Ø­Ø¯ÙŠØ« Ù…Ø¹ Ø£Ø´Ø®Ø§Øµ Ø¬Ø¯Ø¯ØŸ", "difficulty": -1.2, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© ÙÙŠ Ø§Ù„ØªØ¬Ù…Ø¹Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©ØŸ", "difficulty": -0.8, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ Ù‚Ø¶Ø§Ø¡ Ø§Ù„ÙˆÙ‚Øª Ù…Ø¹ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¨Ù‚Ø§Ø¡ ÙˆØ­ÙŠØ¯Ø§Ù‹ØŸ", "difficulty": -0.4, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø­ÙˆØ± Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø§ØªØŸ", "difficulty": 0.5, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªØ¨Ø¯Ø£ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø¨Ø³Ù‡ÙˆÙ„Ø© Ù…Ø¹ Ø§Ù„ØºØ±Ø¨Ø§Ø¡ØŸ", "difficulty": 0.2, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø·Ø§Ù‚Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù†Ø§Ø³ØŸ", "difficulty": -0.6, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„Ø°Ù‡Ø§Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ø­ÙÙ„Ø§Øª ÙˆØ§Ù„Ù…Ù†Ø§Ø³Ø¨Ø§Øª Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©ØŸ", "difficulty": -0.3, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù„ØªØ­Ø¯Ø« Ø£Ù…Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© ÙƒØ¨ÙŠØ±Ø©ØŸ", "difficulty": 0.8, "discrimination": 1.7, "reverse_scored": True},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„ÙØ±Ø¯ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠØ©ØŸ", "difficulty": 1.0, "discrimination": 1.3, "reverse_scored": True},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ù„ÙØª Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† Ø¥Ù„ÙŠÙƒØŸ", "difficulty": 0.7, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© Ø¹Ù†Ø¯ Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø¹Ù† Ø¢Ø±Ø§Ø¦Ùƒ Ø¨ØµÙˆØª Ø¹Ø§Ù„ÙØŸ", "difficulty": 0.4, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù‚Ø´Ø§Øª Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠØ©ØŸ", "difficulty": -0.1, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ù…Ù„Ù„ Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† ÙˆØ­ÙŠØ¯Ø§Ù‹ Ù„ÙØªØ±Ø© Ø·ÙˆÙŠÙ„Ø©ØŸ", "difficulty": -0.5, "discrimination": 1.2},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø£Ø´Ø®Ø§Øµ Ø¬Ø¯Ø¯ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø±ØŸ", "difficulty": 0.1, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ ÙØ±ÙŠÙ‚ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…Ù„ Ø¨Ù…ÙØ±Ø¯ÙƒØŸ", "difficulty": -0.2, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø­ÙŠÙˆÙŠØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø§Øª Ø§Ù„ØµØ§Ø®Ø¨Ø© ÙˆØ§Ù„Ù…Ù„ÙŠØ¦Ø© Ø¨Ø§Ù„Ø­Ø±ÙƒØ©ØŸ", "difficulty": 0.3, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ù‚Ø§Ø¦Ø¯ ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§ØªØŸ", "difficulty": 0.9, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù„Ø§Ø³ØªØ±Ø®Ø§Ø¡ ÙÙŠ Ø§Ù„Ù…Ù†Ø²Ù„ Ø¨Ù…ÙØ±Ø¯ÙƒØŸ", "difficulty": 0.6, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„ØªØ·ÙˆØ¹ÙŠØ© Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠØ©ØŸ", "difficulty": 0.0, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø³Ø¹Ø§Ø¯Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ù…Ø­Ø§Ø·Ø§Ù‹ Ø¨Ø§Ù„Ø£ØµØ¯Ù‚Ø§Ø¡ØŸ", "difficulty": -0.7, "discrimination": 1.2},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªØ­Ø¯Ø« Ø¨ØµØ±Ø§Ø­Ø© Ø¹Ù† Ù…Ø´Ø§Ø¹Ø±Ùƒ Ù…Ø¹ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": 0.2, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ Ù‚Ø¶Ø§Ø¡ Ø¹Ø·Ù„Ø© Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ Ù…Ø¹ Ø§Ù„Ø£ØµØ¯Ù‚Ø§Ø¡ØŸ", "difficulty": -0.4, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø«Ù‚Ø© Ø¹Ù†Ø¯ ØªÙ‚Ø¯ÙŠÙ… Ù†ÙØ³Ùƒ Ù„Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": 0.1, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø§Øª ÙˆØ§Ù„Ø£Ù„Ø¹Ø§Ø¨ Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠØ©ØŸ", "difficulty": 0.0, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© Ø¹Ù†Ø¯ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ù† Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": 0.3, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø­Ø¶ÙˆØ± Ø§Ù„Ù…Ø¤ØªÙ…Ø±Ø§Øª ÙˆØ§Ù„ÙØ¹Ø§Ù„ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©ØŸ", "difficulty": 0.5, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ Ø§Ù„Ø¬Ù„ÙˆØ³ ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø¹Ø¯ Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© ÙÙŠ Ø§Ù„ÙØµÙ„ Ø£Ùˆ Ø§Ù„Ù‚Ø§Ø¹Ø©ØŸ", "difficulty": 0.8, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ù…Ø´Ø§Ø±ÙƒØ© Ø£Ø®Ø¨Ø§Ø±Ùƒ Ø§Ù„Ø´Ø®ØµÙŠØ© Ù…Ø¹ Ø§Ù„Ø£ØµØ¯Ù‚Ø§Ø¡ØŸ", "difficulty": -0.1, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø·Ø§Ù‚Ø© Ø¨Ø¹Ø¯ Ù‚Ø¶Ø§Ø¡ ÙŠÙˆÙ… Ù…Ø¹ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": -0.3, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªÙ†Ø¸ÙŠÙ… Ù„Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©ØŸ", "difficulty": 0.4, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© ÙÙŠ Ø¨ÙŠØ¦Ø§Øª Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ù…ÙØªÙˆØ­Ø©ØŸ", "difficulty": 0.1, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø²Ù…Ù„Ø§Ø¡ Ø§Ù„Ø¹Ù…Ù„ Ø®Ø§Ø±Ø¬ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„ØŸ", "difficulty": 0.2, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ Ø§Ù„Ø°Ù‡Ø§Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø·Ø§Ø¹Ù… Ø§Ù„Ù…Ø²Ø¯Ø­Ù…Ø©ØŸ", "difficulty": 0.3, "discrimination": 1.2},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ù…Ù†Ø§Ù‚Ø´Ø§Øª Ø¹Ù„Ù‰ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØŸ", "difficulty": 0.0, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø³Ø¹Ø§Ø¯Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ø¥Ø¹Ø¬Ø§Ø¨ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": -0.2, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„Ø³ÙØ± Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø£ØµØ¯Ù‚Ø§Ø¡ØŸ", "difficulty": -0.1, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ Ø§Ù„ØªØ³ÙˆÙ‚ Ù…Ø¹ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„ØªØ³ÙˆÙ‚ Ø¨Ù…ÙØ±Ø¯ÙƒØŸ", "difficulty": 0.0, "discrimination": 1.2},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø­Ø¶ÙˆØ± Ø­ÙÙ„Ø§Øª Ø£Ø¹ÙŠØ§Ø¯ Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯ ÙˆØ§Ù„Ù…Ù†Ø§Ø³Ø¨Ø§ØªØŸ", "difficulty": -0.6, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© Ø¹Ù†Ø¯ Ø§Ù„ØªØ­Ø¯Ø« ÙÙŠ Ø§Ù„Ù‡Ø§ØªÙ Ù…Ø¹ Ø£Ø´Ø®Ø§Øµ Ù„Ø§ ØªØ¹Ø±ÙÙ‡Ù… Ø¬ÙŠØ¯Ø§Ù‹ØŸ", "difficulty": 0.7, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠØ©ØŸ", "difficulty": 0.1, "discrimination": 1.4}
            ],
            "agreeableness": [
                {"text": "Ù‡Ù„ ØªØ­Ø§ÙˆÙ„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† Ø¹Ù†Ø¯Ù…Ø§ ÙŠØ­ØªØ§Ø¬ÙˆÙ† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©ØŸ", "difficulty": -1.0, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ«Ù‚ ÙÙŠ Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ù†Ø§Ø³ Ø§Ù„Ø·ÙŠØ¨Ø©ØŸ", "difficulty": -0.5, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªØ¹Ø§ÙˆÙ† Ù…Ø¹ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": -0.8, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„ØªØ¹Ø§Ø·Ù Ù…Ø¹ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹Ø§Ù†ÙˆÙ†ØŸ", "difficulty": -1.2, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ ØªØ¬Ù†Ø¨ Ø§Ù„ØµØ±Ø§Ø¹Ø§Øª ÙˆØ§Ù„Ù…Ø´Ø§ÙƒÙ„ØŸ", "difficulty": -0.3, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† ÙÙŠ Ø£ÙØ±Ø§Ø­Ù‡Ù… ÙˆØ£Ø­Ø²Ø§Ù†Ù‡Ù…ØŸ", "difficulty": -0.6, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø±ÙØ¶ Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©ØŸ", "difficulty": 0.2, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„ØªØ·ÙˆØ¹ÙŠ ÙˆÙ…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ØŸ", "difficulty": 0.1, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø°Ù†Ø¨ Ø¹Ù†Ø¯Ù…Ø§ ØªØ±ÙØ¶ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø´Ø®Øµ Ù…Ø§ØŸ", "difficulty": 0.0, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªÙˆØ³Ø· ÙÙŠ Ø­Ù„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø¨ÙŠÙ† Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": 0.4, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ Ø§Ù„ØªÙØ§ÙˆØ¶ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø¬Ø¯Ø§Ù„ØŸ", "difficulty": -0.1, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø³Ø¹Ø§Ø¯Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªØ³Ø§Ø¹Ø¯ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": -0.9, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† ÙˆÙ…Ø­Ø§ÙˆÙ„Ø© Ø­Ù„Ù‡Ø§ØŸ", "difficulty": -0.2, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ«Ù‚ Ø¨Ø³Ù‡ÙˆÙ„Ø© ÙÙŠ Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ø¬Ø¯Ø¯ØŸ", "difficulty": 0.3, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ù…Ø´Ø§Ø±ÙƒØ© Ù…Ù…ØªÙ„ÙƒØ§ØªÙƒ Ù…Ø¹ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": 0.5, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© Ø¹Ù†Ø¯ Ø§Ù„ØªÙ†Ø§Ø²Ù„ Ø¹Ù† Ø­Ù‚ÙˆÙ‚Ùƒ Ù„Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": 0.8, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": -0.4, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ù…ØµÙ„Ø­Ø© Ø§Ù„Ø¬Ù…Ø§Ø¹Ø© Ù‚Ø¨Ù„ Ù…ØµÙ„Ø­ØªÙƒØŸ", "difficulty": 0.2, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù†ØªÙ‚Ø§Ø¯ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†ÙˆØ§ Ù…Ø®Ø·Ø¦ÙŠÙ†ØŸ", "difficulty": 0.1, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": -0.3, "discrimination": 1.2},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø§Ù…ØªÙ†Ø§Ù† Ø¨Ø³Ù‡ÙˆÙ„Ø© ØªØ¬Ø§Ù‡ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": -0.5, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ø­ØªØ±Ø§Ù… Ø¢Ø±Ø§Ø¡ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† Ø­ØªÙ‰ Ù„Ùˆ Ø§Ø®ØªÙ„ÙØª Ù…Ø¹Ù‡Ø§ØŸ", "difficulty": -0.1, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ Ø§Ù„Ø¹ÙÙˆ ÙˆØ§Ù„Ù…Ø³Ø§Ù…Ø­Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù…ØŸ", "difficulty": 0.0, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªÙ‡Ù†Ø¦Ø© Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† Ø¹Ù„Ù‰ Ø¥Ù†Ø¬Ø§Ø²Ø§ØªÙ‡Ù…ØŸ", "difficulty": -0.7, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© Ø¹Ù†Ø¯ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„ØªÙ†Ø§Ø²Ù„Ø§Øª ÙÙŠ Ø§Ù„Ù†Ù‚Ø§Ø´Ø§ØªØŸ", "difficulty": 0.3, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„ÙˆØ¯ ÙˆØ§Ù„ØµØ¯Ø§Ù‚Ø© Ù„Ù„Ø¬Ù…ÙŠØ¹ØŸ", "difficulty": -0.4, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ ØªØ¬Ù†Ø¨ Ø¥ÙŠØ°Ø§Ø¡ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": -0.6, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ø¨ÙŠØ¦Ø© ØªØ¹Ø§ÙˆÙ†ÙŠØ© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„ØªÙ†Ø§ÙØ³ÙŠØ©ØŸ", "difficulty": -0.2, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø³Ø¹Ø§Ø¯Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªØ±Ù‰ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† Ø³Ø¹Ø¯Ø§Ø¡ØŸ", "difficulty": -0.8, "discrimination": 1.2},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªØ¬Ù†Ø¨ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ù…Ø«ÙŠØ±Ø© Ù„Ù„Ø¬Ø¯Ù„ØŸ", "difficulty": 0.1, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ Ø§Ù„ØµØ¨Ø± ÙˆØ§Ù„ØªÙÙ‡Ù… Ù…Ø¹ Ø§Ù„Ø£Ø´Ø®Ø§Øµ ØµØ¹Ø¨ÙŠ Ø§Ù„Ù…Ø±Ø§Ø³ØŸ", "difficulty": 0.4, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ù…Ø±Ø§Ø¹Ø§Ø© Ø¸Ø±ÙˆÙ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† Ø¹Ù†Ø¯ Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§ØªØŸ", "difficulty": -0.1, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© Ø¹Ù†Ø¯ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø§Ø¹ØªØ°Ø§Ø± Ø­ØªÙ‰ Ù„Ùˆ Ù„Ù… ØªÙƒÙ† Ù…Ø®Ø·Ø¦Ø§Ù‹ØŸ", "difficulty": 0.6, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„ØªÙ‚Ø¯ÙŠØ± ÙˆØ§Ù„Ø´ÙƒØ± Ù„Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": -0.5, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ Ø­Ù„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø¨Ø§Ù„Ø­ÙˆØ§Ø± Ø§Ù„Ù‡Ø§Ø¯Ø¦ØŸ", "difficulty": -0.3, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù† Ø°Ù„Ùƒ Ø¹Ù„Ù‰ Ø­Ø³Ø§Ø¨ ÙˆÙ‚ØªÙƒØŸ", "difficulty": 0.2, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© Ø¹Ù†Ø¯ Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ù†Ù‚Ø¯ Ø§Ù„Ø¨Ù†Ø§Ø¡ØŸ", "difficulty": 0.0, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„ØªØ³Ø§Ù…Ø­ Ù…Ø¹ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": -0.2, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªÙØ¶Ù„ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© ÙÙŠ Ø§Ù„Ø£Ø´Ø®Ø§ØµØŸ", "difficulty": -0.4, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªØ¬Ù†Ø¨ Ø§Ù„Ù…ÙˆØ§Ù‚Ù Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªØ¤Ø°ÙŠ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": -0.3, "discrimination": 1.5}
            ],
            "conscientiousness": [
                {"text": "Ù‡Ù„ ØªØ®Ø·Ø· Ù„Ø£Ø¹Ù…Ø§Ù„Ùƒ Ù…Ø³Ø¨Ù‚Ø§Ù‹ØŸ", "difficulty": -0.5, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø±Øµ Ø¹Ù„Ù‰ Ø¥Ù†Ø¬Ø§Ø² Ù…Ù‡Ø§Ù…Ùƒ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø­Ø¯Ø¯ØŸ", "difficulty": -0.8, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„ØªØ±ØªÙŠØ¨ØŸ", "difficulty": -0.3, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ¶Ø¹ Ø£Ù‡Ø¯Ø§ÙØ§Ù‹ ÙˆØ§Ø¶Ø­Ø© Ù„Ù†ÙØ³ÙƒØŸ", "difficulty": -0.1, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ­Ø±Øµ Ø¹Ù„Ù‰ Ø¥ØªÙ…Ø§Ù… Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙŠ ØªØ¨Ø¯Ø£Ù‡Ø§ØŸ", "difficulty": -0.6, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø³Ø¨Ù‚ Ù„Ù„Ø§Ù…ØªØ­Ø§Ù†Ø§Øª ÙˆØ§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø§ØªØŸ", "difficulty": -0.4, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ù…Ù†Ø¸Ù…Ø© Ø­ÙˆÙ„ÙƒØŸ", "difficulty": -0.2, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ­Ø±Øµ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØµÙˆÙ„ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø­Ø¯Ø¯ Ù„Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ØŸ", "difficulty": -0.7, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ¤Ø¬Ù„ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ù‡Ù…Ø© Ù„Ù„Ø­Ø¸Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©ØŸ", "difficulty": 0.8, "discrimination": 1.7, "reverse_scored": True},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ÙˆØ¶Ø¹ Ù‚ÙˆØ§Ø¦Ù… Ø¨Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø¥Ù†Ø¬Ø§Ø²Ù‡Ø§ØŸ", "difficulty": 0.1, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø±Øµ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© ÙˆØ§Ù„ØªØ¯Ù‚ÙŠÙ‚ ÙÙŠ Ø¹Ù…Ù„ÙƒØŸ", "difficulty": -0.1, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ù‚Ù„Ù‚ Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ø£Ù…ÙˆØ± ØºÙŠØ± Ù…Ù†Ø¸Ù…Ø©ØŸ", "difficulty": 0.2, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ÙˆØ¶Ø¹ Ø¬Ø¯ÙˆÙ„ Ø²Ù…Ù†ÙŠ Ù„Ø£Ù†Ø´Ø·ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ÙŠØ©ØŸ", "difficulty": 0.3, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ­Ø±Øµ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù…Ù…ØªÙ„ÙƒØ§ØªÙƒ ÙÙŠ Ø£Ù…Ø§ÙƒÙ†Ù‡Ø§ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©ØŸ", "difficulty": 0.0, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø¶Ø§ Ø¹Ù†Ø¯ Ø¥Ù†Ø¬Ø§Ø² Ø§Ù„Ù…Ù‡Ø§Ù… Ø¨Ø¯Ù‚Ø© ÙˆØ¥ØªÙ‚Ø§Ù†ØŸ", "difficulty": -0.4, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªØ®Ø·ÙŠØ· Ù„Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø·ÙˆÙŠÙ„ØŸ", "difficulty": 0.1, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø±Øµ Ø¹Ù„Ù‰ Ø§ØªØ¨Ø§Ø¹ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ…Ø§ØªØŸ", "difficulty": -0.2, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ù…Ù‡Ù…Ø© ÙˆØ§Ø­Ø¯Ø© Ù„ÙØªØ±Ø© Ø·ÙˆÙŠÙ„Ø©ØŸ", "difficulty": 0.5, "discrimination": 1.7, "reverse_scored": True},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªÙ†Ø¸ÙŠÙ… Ù…Ù„ÙØ§ØªÙƒ ÙˆÙˆØ«Ø§Ø¦Ù‚Ùƒ Ø¨Ø´ÙƒÙ„ Ù…Ù†ØªØ¸Ù…ØŸ", "difficulty": 0.2, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø±Øµ Ø¹Ù„Ù‰ Ø¥Ù†Ù‡Ø§Ø¡ Ø¹Ù…Ù„Ùƒ Ù‚Ø¨Ù„ Ø§Ù„Ø§Ø³ØªØ±Ø§Ø­Ø© Ø£Ùˆ Ø§Ù„Ù„Ø¹Ø¨ØŸ", "difficulty": 0.0, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø¥Ø­Ø¨Ø§Ø· Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ØªÙ†Ø¬Ø² Ù…Ø§ Ø®Ø·Ø·Øª Ù„Ù‡ØŸ", "difficulty": -0.1, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø®Ø·Ø·Ùƒ ÙˆØªØ¹Ø¯ÙŠÙ„Ù‡Ø§ Ø¨Ø§Ù†ØªØ¸Ø§Ù…ØŸ", "difficulty": 0.4, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ­Ø±Øµ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙŠÙ‚Ø§Ø¸ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙƒÙ„ ÙŠÙˆÙ…ØŸ", "difficulty": -0.3, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø¥Ù†Ø¬Ø§Ø² Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØµØ¹Ø¨Ø© Ø£ÙˆÙ„Ø§Ù‹ØŸ", "difficulty": 0.6, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© Ø¹Ù†Ø¯ Ø§ØªØ¨Ø§Ø¹ Ø±ÙˆØªÙŠÙ† ÙŠÙˆÙ…ÙŠ Ø«Ø§Ø¨ØªØŸ", "difficulty": 0.1, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø±Øµ Ø¹Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙŠ Ù…ÙƒØ§Ù† Ø¢Ù…Ù†ØŸ", "difficulty": -0.2, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ØŸ", "difficulty": -0.1, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø°Ù†Ø¨ Ø¹Ù†Ø¯Ù…Ø§ ØªØ¶ÙŠØ¹ ÙˆÙ‚ØªÙƒ Ø¨Ù„Ø§ ÙØ§Ø¦Ø¯Ø©ØŸ", "difficulty": 0.0, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ÙˆØ¶Ø¹ Ù…ÙˆØ§Ø¹ÙŠØ¯ Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ø¥Ù†Ø¬Ø§Ø² Ù…Ù‡Ø§Ù…ÙƒØŸ", "difficulty": 0.2, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø±Øµ Ø¹Ù„Ù‰ ØªÙ†Ø¸ÙŠÙ Ù…ÙƒØ§Ù† Ø¹Ù…Ù„Ùƒ Ø£Ùˆ Ø¯Ø±Ø§Ø³ØªÙƒ Ø¨Ø§Ù†ØªØ¸Ø§Ù…ØŸ", "difficulty": 0.1, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙ…Ù„ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù‡Ø§Ù…Ùƒ Ø§Ù„ÙŠÙˆÙ…ÙŠØ©ØŸ", "difficulty": -0.3, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ø³Ø¨Ù‚ Ù„Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„ÙƒØ¨ÙŠØ±Ø©ØŸ", "difficulty": 0.0, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ­Ø±Øµ Ø¹Ù„Ù‰ ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„Ø¬ÙŠØ¯ØŸ", "difficulty": -0.1, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„ØªÙˆØªØ± Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ù„Ø¯ÙŠÙƒ Ù…Ù‡Ø§Ù… ØºÙŠØ± Ù…Ù†Ø¬Ø²Ø©ØŸ", "difficulty": 0.1, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¥Ù„Ù‰ Ø®Ø·ÙˆØ§Øª ØµØºÙŠØ±Ø©ØŸ", "difficulty": 0.3, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø±Øµ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù„Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© ÙÙŠ Ø¹Ù…Ù„ÙƒØŸ", "difficulty": 0.2, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªÙ†Ø¬Ø² Ø£ÙƒØ«Ø± Ù…Ù…Ø§ Ø®Ø·Ø·Øª Ù„Ù‡ØŸ", "difficulty": -0.2, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ù…Ø±Ø§Ù‚Ø¨Ø© ØªÙ‚Ø¯Ù…Ùƒ Ù†Ø­Ùˆ ØªØ­Ù‚ÙŠÙ‚ Ø£Ù‡Ø¯Ø§ÙÙƒØŸ", "difficulty": 0.1, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ­Ø±Øµ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø© ÙÙŠ ÙŠÙˆÙ…ÙƒØŸ", "difficulty": 0.5, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„ÙØ®Ø± Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙ…Ø¯Ø­ Ø§Ù„Ø¢Ø®Ø±ÙˆÙ† Ø§Ù†Ø¶Ø¨Ø§Ø·Ùƒ ÙˆØªÙ†Ø¸ÙŠÙ…ÙƒØŸ", "difficulty": -0.1, "discrimination": 1.4}
            ],
            "neuroticism": [
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ù‚Ù„Ù‚ Ø¨Ø³Ù‡ÙˆÙ„Ø©ØŸ", "difficulty": -0.8, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØªØ£Ø«Ø± Ø¨Ø§Ù„Ø¶ØºÙˆØ· Ø§Ù„Ù†ÙØ³ÙŠØ© Ø¨Ø³Ø±Ø¹Ø©ØŸ", "difficulty": -0.5, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù…Ø´Ø§Ø¹Ø±Ùƒ Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ØŸ", "difficulty": -0.3, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„ØªÙˆØªØ± Ù‚Ø¨Ù„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©ØŸ", "difficulty": -0.6, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªÙ‚Ù„Ù‚ Ø¨Ø´Ø£Ù† Ø£Ø´ÙŠØ§Ø¡ Ù‚Ø¯ Ù„Ø§ ØªØ­Ø¯Ø« Ø£Ø¨Ø¯Ø§Ù‹ØŸ", "difficulty": -0.1, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø­Ø²Ù† Ø£Ùˆ Ø§Ù„Ø§ÙƒØªØ¦Ø§Ø¨ Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ Ø¨Ø¯ÙˆÙ† Ø³Ø¨Ø¨ ÙˆØ§Ø¶Ø­ØŸ", "difficulty": 0.2, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù„Ø§Ø³ØªØ±Ø®Ø§Ø¡ ÙˆØ§Ù„Ù‡Ø¯ÙˆØ¡ØŸ", "difficulty": 0.0, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„ØºØ¶Ø¨ Ø¨Ø³Ø±Ø¹Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªÙˆØ§Ø¬Ù‡ Ù…Ø´Ø§ÙƒÙ„ØŸ", "difficulty": -0.2, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ®Ø§Ù Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ù‚Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø£Ùˆ ØºÙŠØ± Ø§Ù„Ù…Ø£Ù„ÙˆÙØ©ØŸ", "difficulty": 0.1, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø¹Ø¯Ù… Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ù‚Ø¯Ø±Ø§ØªÙƒ Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ØŸ", "difficulty": -0.4, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù„Ù†ÙˆÙ… Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ù‚Ù„Ù‚Ø§Ù‹ØŸ", "difficulty": -0.3, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø®ÙˆÙ Ù…Ù† Ø§Ù„ÙØ´Ù„ ÙÙŠ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ù‡Ù…Ø©ØŸ", "difficulty": -0.1, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØªÙØ§Ø¹Ù„ Ø¨Ù‚ÙˆØ© Ù…Ø¹ Ø§Ù„Ù†Ù‚Ø¯ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù† Ø¨Ù†Ø§Ø¡Ù‹ØŸ", "difficulty": 0.3, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø¥Ø±Ù‡Ø§Ù‚ Ø§Ù„Ù†ÙØ³ÙŠ Ø¨Ø³Ù‡ÙˆÙ„Ø©ØŸ", "difficulty": 0.0, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªÙ‚Ù„Ù‚ Ø¨Ø´Ø£Ù† Ù…Ø§ ÙŠÙÙƒØ± Ø¨Ù‡ Ø§Ù„Ø¢Ø®Ø±ÙˆÙ† Ø¹Ù†ÙƒØŸ", "difficulty": -0.2, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„ØªÙˆØªØ± ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ù‚Ù Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©ØŸ", "difficulty": -0.1, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©ØŸ", "difficulty": 0.2, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø®ÙˆÙ Ù…Ù† Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª ÙÙŠ Ø­ÙŠØ§ØªÙƒØŸ", "difficulty": 0.1, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªÙ„ÙˆÙ… Ù†ÙØ³Ùƒ ÙƒØ«ÙŠØ±Ø§Ù‹ Ø¹Ù†Ø¯Ù…Ø§ ØªØ­Ø¯Ø« Ø£Ø®Ø·Ø§Ø¡ØŸ", "difficulty": -0.3, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ù‚Ù„Ù‚ Ø¨Ø´Ø£Ù† ØµØ­ØªÙƒ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ù„Ø§Ø²Ù…ØŸ", "difficulty": 0.4, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…ÙˆØ§Ù‚Ù Ø§Ù„Ù…ÙØ§Ø¬Ø¦Ø©ØŸ", "difficulty": 0.0, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© ØªØ¬Ø§Ù‡ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": -0.1, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ®Ø§Ù Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ ÙˆÙ…Ø§ Ù‚Ø¯ ÙŠØ­Ù…Ù„Ù‡ Ù…Ù† Ù…Ø´Ø§ÙƒÙ„ØŸ", "difficulty": 0.2, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø°Ù†Ø¨ Ø¨Ø³Ù‡ÙˆÙ„Ø© Ø­ØªÙ‰ ÙÙŠ Ø§Ù„Ø£Ù…ÙˆØ± Ø§Ù„Ø¨Ø³ÙŠØ·Ø©ØŸ", "difficulty": 0.1, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø¹Ù† Ù…Ø´Ø§Ø¹Ø±Ùƒ Ø¨ÙˆØ¶ÙˆØ­ØŸ", "difficulty": 0.0, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„ÙˆØ­Ø¯Ø© Ø­ØªÙ‰ Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ù…Ø¹ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": 0.5, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªÙ‚Ù„Ù‚ Ø¨Ø´Ø£Ù† Ø£Ø¯Ø§Ø¦Ùƒ ÙÙŠ Ø§Ù„Ø¹Ù…Ù„ Ø£Ùˆ Ø§Ù„Ø¯Ø±Ø§Ø³Ø© Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø±ØŸ", "difficulty": -0.2, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø¥Ø­Ø¨Ø§Ø· Ø¨Ø³Ø±Ø¹Ø© Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ØªØ­Ù‚Ù‚ Ù…Ø§ ØªØ±ÙŠØ¯ØŸ", "difficulty": -0.1, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ®Ø§Ù Ù…Ù† Ø¥Ø¨Ø¯Ø§Ø¡ Ø±Ø£ÙŠÙƒ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ù…Ø«ÙŠØ±Ø© Ù„Ù„Ø¬Ø¯Ù„ØŸ", "difficulty": 0.3, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø£Ù† Ø§Ù„Ø­ÙŠØ§Ø© ØµØ¹Ø¨Ø© ÙˆÙ…Ù„ÙŠØ¦Ø© Ø¨Ø§Ù„ØªØ­Ø¯ÙŠØ§ØªØŸ", "difficulty": 0.1, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø±Ø¯ÙˆØ¯ Ø£ÙØ¹Ø§Ù„Ùƒ Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©ØŸ", "difficulty": 0.0, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø®ÙˆÙ Ù…Ù† ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø£Ø´Ø®Ø§Øµ Ø§Ù„Ù…Ù‡Ù…ÙŠÙ† ÙÙŠ Ø­ÙŠØ§ØªÙƒØŸ", "difficulty": -0.3, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªÙ‚Ù„Ù‚ Ø¨Ø´Ø£Ù† Ø§Ù„Ø£Ù…ÙˆØ± Ø§Ù„Ù…Ø§Ù„ÙŠØ© Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ù„Ø§Ø²Ù…ØŸ", "difficulty": 0.2, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„ØªÙˆØªØ± Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† ØªØ­Øª Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ùˆ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…ØŸ", "difficulty": -0.1, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ù†Ø³ÙŠØ§Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø£Ùˆ Ø§Ù„Ø¥Ø­Ø±Ø§Ø¬Ø§Øª Ø§Ù„Ù…Ø§Ø¶ÙŠØ©ØŸ", "difficulty": 0.1, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø£Ù† Ø§Ù„Ø¹Ø§Ù„Ù… Ù…ÙƒØ§Ù† Ø®Ø·ÙŠØ± ÙˆÙ…Ù„ÙŠØ¡ Ø¨Ø§Ù„ØªÙ‡Ø¯ÙŠØ¯Ø§ØªØŸ", "difficulty": 0.6, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªÙ‚Ù„Ù‚ Ø¨Ø´Ø£Ù† ÙƒÙŠÙÙŠØ© ØªØ£Ø«ÙŠØ± Ù‚Ø±Ø§Ø±Ø§ØªÙƒ Ø¹Ù„Ù‰ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†ØŸ", "difficulty": 0.0, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ ÙƒÙ„ Ø´ÙŠØ¡ Ø­ÙˆÙ„ÙƒØŸ", "difficulty": 0.3, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù„Ø«Ù‚Ø© Ø¨Ø£Ù† Ø§Ù„Ø£Ù…ÙˆØ± Ø³ØªØ³ÙŠØ± Ø¹Ù„Ù‰ Ù…Ø§ ÙŠØ±Ø§Ù…ØŸ", "difficulty": 0.2, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ´Ø¹Ø± Ø¨Ø§Ù„Ù‚Ù„Ù‚ Ø§Ù„Ø²Ø§Ø¦Ø¯ Ø¨Ø´Ø£Ù† Ø³Ù„Ø§Ù…Ø© Ø£Ø­Ø¨Ø§Ø¦ÙƒØŸ", "difficulty": -0.2, "discrimination": 1.7}
            ],
            "openness": [
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªØ¬Ø±Ø¨Ø© Ø£Ø´ÙŠØ§Ø¡ Ø¬Ø¯ÙŠØ¯Ø©ØŸ", "difficulty": -0.8, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ©ØŸ", "difficulty": -0.5, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ø£ÙÙƒØ§Ø± Ù…Ø¬Ø±Ø¯Ø©ØŸ", "difficulty": 0.2, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ÙƒØªØ¨ ÙˆØ§Ù„Ù‚ØµØµØŸ", "difficulty": -0.3, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ø³ØªÙƒØ´Ø§Ù Ø£Ù…Ø§ÙƒÙ† Ø¬Ø¯ÙŠØ¯Ø©ØŸ", "difficulty": -0.6, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ ÙˆØ§Ù„ÙÙ†ÙˆÙ†ØŸ", "difficulty": -0.4, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªØ¹Ù„Ù… Ù„ØºØ§Øª Ø¬Ø¯ÙŠØ¯Ø©ØŸ", "difficulty": 0.1, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„Ù…Ù†Ø§Ù‚Ø´Ø§Øª Ø§Ù„ÙÙ„Ø³ÙÙŠØ© ÙˆØ§Ù„ÙÙƒØ±ÙŠØ©ØŸ", "difficulty": 0.4, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªØ¬Ø±Ø¨Ø© Ø£Ø·Ø¹Ù…Ø© Ù…Ù† Ø«Ù‚Ø§ÙØ§Øª Ù…Ø®ØªÙ„ÙØ©ØŸ", "difficulty": -0.2, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø­Ù„ Ø§Ù„Ø£Ù„ØºØ§Ø² ÙˆØ§Ù„Ù…Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©ØŸ", "difficulty": 0.0, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ù…Ø´Ø§Ù‡Ø¯Ø© Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„ÙÙ†ÙŠØ© Ø£Ùˆ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠØ©ØŸ", "difficulty": 0.3, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨ØªØ¹Ù„Ù… Ù…Ù‡Ø§Ø±Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø±ØŸ", "difficulty": -0.1, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© Ù„Ù„Ø­ÙŠØ§Ø©ØŸ", "difficulty": 0.2, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ù…ØªØ§Ø­Ù ÙˆØ§Ù„Ù…Ø¹Ø§Ø±Ø¶ Ø§Ù„ÙÙ†ÙŠØ©ØŸ", "difficulty": 0.5, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø´Ø¹Ø± ÙˆØ§Ù„Ø£Ø¯Ø¨ØŸ", "difficulty": 0.6, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ù†Ø¸Ø±ÙŠØ§Øª Ø¹Ù„Ù…ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©ØŸ", "difficulty": 0.7, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªØ¬Ø±Ø¨Ø© Ø·Ø±Ù‚ Ø¬Ø¯ÙŠØ¯Ø© Ù„Ø­Ù„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ØŸ", "difficulty": -0.1, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„ØªØ£Ù…Ù„ ÙÙŠ Ø¬Ù…Ø§Ù„ Ø§Ù„Ø·Ø¨ÙŠØ¹Ø©ØŸ", "difficulty": -0.4, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§ÙƒØªØ´Ø§Ù Ø«Ù‚Ø§ÙØ§Øª ÙˆØªÙ‚Ø§Ù„ÙŠØ¯ Ø¬Ø¯ÙŠØ¯Ø©ØŸ", "difficulty": 0.0, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„ØªÙÙƒÙŠØ± Ø®Ø§Ø±Ø¬ Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ØŸ", "difficulty": 0.1, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªØµÙ…ÙŠÙ… Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø´ÙŠØ§Ø¡ Ø¨ÙŠØ¯ÙŠÙƒØŸ", "difficulty": 0.2, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ø£Ø³Ø¦Ù„Ø© ÙˆØ¬ÙˆØ¯ÙŠØ© Ø¹Ù…ÙŠÙ‚Ø©ØŸ", "difficulty": 0.8, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªØ¬Ø±Ø¨Ø© Ø£Ù†Ø´Ø·Ø© Ù…ØºØ§Ù…Ø±Ø© ÙˆØ¬Ø±ÙŠØ¦Ø©ØŸ", "difficulty": 0.3, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù‚ØµØµ Ø£Ùˆ Ø§Ù„Ø´Ø¹Ø±ØŸ", "difficulty": 0.9, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø¯Ø±Ø§Ø³Ø© Ù…ÙˆØ§Ø¶ÙŠØ¹ ØºÙŠØ± ØªÙ‚Ù„ÙŠØ¯ÙŠØ©ØŸ", "difficulty": 0.4, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ù…Ø®ØªÙ„ÙØ©ØŸ", "difficulty": 0.1, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø¹Ù† Ù†ÙØ³Ùƒ Ø¨Ø·Ø±Ù‚ Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ©ØŸ", "difficulty": 0.0, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ù…Ù†Ø§Ù‚Ø´Ø© Ø£ÙÙƒØ§Ø± Ø¬Ø¯ÙŠØ¯Ø© ÙˆÙ…Ø¨ØªÙƒØ±Ø©ØŸ", "difficulty": 0.2, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªØ­Ø¯ÙŠ Ø§Ù„ØªÙ‚Ø§Ù„ÙŠØ¯ ÙˆØ§Ù„Ø£Ø¹Ø±Ø§Ù Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©ØŸ", "difficulty": 1.0, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù…ÙˆØ§Ø¶ÙŠØ¹ Ù…Ø¹Ù‚Ø¯Ø© ÙˆÙ…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø¬ÙˆØ§Ù†Ø¨ØŸ", "difficulty": 0.5, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø±ÙŠØ§ØªÙƒ Ø§Ù„Ø®Ø§ØµØ© Ø­ÙˆÙ„ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ØŸ", "difficulty": 0.6, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„ØªØ¬Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§ÙƒØªØ´Ø§ÙØŸ", "difficulty": -0.2, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ø¹Ù„Ø§Ù‚Ø§Øª Ù…Ø¹Ù‚Ø¯Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ØŸ", "difficulty": 0.3, "discrimination": 1.7},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„Ø­ÙˆØ§Ø±Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ­Ø¯Ù‰ Ø·Ø±ÙŠÙ‚Ø© ØªÙÙƒÙŠØ±ÙƒØŸ", "difficulty": 0.4, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ø³ØªÙƒØ´Ø§Ù Ø£ÙÙƒØ§Ø± ÙˆØ¢Ø±Ø§Ø¡ Ù…Ø®ØªÙ„ÙØ© Ø¹Ù† Ø¢Ø±Ø§Ø¦ÙƒØŸ", "difficulty": 0.1, "discrimination": 1.8},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ù…Ø¹Ø§Ù†ÙŠ Ø±Ù…Ø²ÙŠØ© ÙˆÙ…Ø¬Ø§Ø²ÙŠØ©ØŸ", "difficulty": 0.7, "discrimination": 1.4},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ ØªØ¬Ø±Ø¨Ø© ØªÙ‚Ù†ÙŠØ§Øª Ø£Ùˆ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©ØŸ", "difficulty": -0.1, "discrimination": 1.5},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø¯Ø±Ø§Ø³Ø© Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„Ø­Ø¶Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©ØŸ", "difficulty": 0.2, "discrimination": 1.3},
                {"text": "Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ ÙƒÙŠÙÙŠØ© ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¹Ø§Ù„Ù… Ù…Ù† Ø­ÙˆÙ„ÙƒØŸ", "difficulty": 0.0, "discrimination": 1.6},
                {"text": "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ Ø¨Ø§Ù„Ø³ÙØ± Ù„Ø§ÙƒØªØ´Ø§Ù Ø«Ù‚Ø§ÙØ§Øª Ø¬Ø¯ÙŠØ¯Ø©ØŸ", "difficulty": 0.1, "discrimination": 1.4}
            ]
        }
        
        # Get questions for the dimension
        base_questions = question_banks.get(dimension, [])
        
        # Personalize questions based on demographics
        personalized_questions = []
        dimension_prefix = {
            "extraversion": "e",
            "agreeableness": "a", 
            "conscientiousness": "c",
            "neuroticism": "n",
            "openness": "o"
        }
        
        prefix = dimension_prefix.get(dimension, dimension[0])
        
        # Shuffle questions for randomization and ensure uniqueness
        base_questions_copy = base_questions.copy()
        random.shuffle(base_questions_copy)
        
        # Track used questions to avoid duplicates
        used_questions = set()
        
        for i, q in enumerate(base_questions_copy[:count]):
            # Skip if question text already used
            if q["text"] in used_questions:
                continue
                
            used_questions.add(q["text"])
            
            personalized_text = self._personalize_question_text(
                q["text"], demographics
            )
            
            question = {
                "question_id": f"{prefix}{i+1}",
                "text": personalized_text,
                "difficulty": q["difficulty"],
                "discrimination": q["discrimination"],
                "reverse_scored": q.get("reverse_scored", False),
                "subdimension": "Ø¹Ø§Ù…",
                "dimension": dimension,
                "demographic_context": demographics
            }
            personalized_questions.append(question)
        
        return personalized_questions
    
    def _personalize_question_text(self, text, demographics):
        """Personalize question text based on demographics"""
        gender = demographics.get("gender", "")
        
        # Apply gender-specific grammar
        if gender == "female":
            # Convert to feminine forms
            text = text.replace("Ù‡Ù„ ØªØ­Ø¨", "Ù‡Ù„ ØªØ­Ø¨ÙŠÙ†")
            text = text.replace("Ù‡Ù„ ØªØ´Ø¹Ø±", "Ù‡Ù„ ØªØ´Ø¹Ø±ÙŠÙ†")
            text = text.replace("Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹", "Ù‡Ù„ ØªØ³ØªÙ…ØªØ¹ÙŠÙ†")
            text = text.replace("Ù‡Ù„ ØªØ­Ø±Øµ", "Ù‡Ù„ ØªØ­Ø±ØµÙŠÙ†")
            text = text.replace("Ù‡Ù„ ØªØ®Ø·Ø·", "Ù‡Ù„ ØªØ®Ø·Ø·ÙŠÙ†")
            text = text.replace("Ù‡Ù„ ØªØ«Ù‚", "Ù‡Ù„ ØªØ«Ù‚ÙŠÙ†")
            text = text.replace("Ù‡Ù„ ØªØ¬Ø¯", "Ù‡Ù„ ØªØ¬Ø¯ÙŠÙ†")
            text = text.replace("Ù‡Ù„ ØªÙØ¶Ù„", "Ù‡Ù„ ØªÙØ¶Ù„ÙŠÙ†")
            text = text.replace("Ù‡Ù„ ØªØ­Ø§ÙˆÙ„", "Ù‡Ù„ ØªØ­Ø§ÙˆÙ„ÙŠÙ†")
            text = text.replace("Ù‡Ù„ ØªÙ‚Ù„Ù‚", "Ù‡Ù„ ØªÙ‚Ù„Ù‚ÙŠÙ†")
        
        return text
    
    async def _generate_ai_questions_background(self, demographics, dimension, cache_key):
        """Generate AI questions in background for future use"""
        try:
            print(f"ðŸ¤– Generating AI questions in background for {dimension}")
            # This will run in background without blocking user experience
            await self._generate_ai_questions_slow(demographics, dimension, cache_key)
        except Exception as e:
            print(f"Background AI generation failed: {e}")
    
    async def _generate_ai_questions_slow(self, demographics, dimension, cache_key):
        """Original AI generation method (moved to background)"""
        # ... original Groq generation code would go here
        pass

# Advanced IRT and CAT Implementation
class AdaptiveTestEngine:
    def __init__(self):
        self.irt_params = IRTParameters()
    
    def calculate_item_information(self, theta, difficulty, discrimination):
        """Calculate item information function"""
        try:
            # 2PL IRT model
            z = discrimination * (theta - difficulty)
            p = 1 / (1 + np.exp(-z))
            q = 1 - p
            information = discrimination**2 * p * q
            return information
        except:
            return 0.1  # Fallback value
    
    def estimate_theta(self, responses, difficulties, discriminations):
        """Estimate theta using Maximum Likelihood Estimation"""
        if not responses:
            return 0.0, 1.0  # Initial theta and SE
        
        # MLE estimation
        theta = 0.0
        for iteration in range(50):  # Maximum iterations
            likelihood_derivative = 0
            information_sum = 0
            
            for i, response in enumerate(responses):
                difficulty = difficulties[i]
                discrimination = discriminations[i]
                
                z = discrimination * (theta - difficulty)
                p = 1 / (1 + np.exp(-z))
                
                # First derivative (score function)
                likelihood_derivative += discrimination * (response - p)
                
                # Second derivative (information)
                information_sum += discrimination**2 * p * (1 - p)
            
            # Newton-Raphson update
            if information_sum > 0:
                theta_new = theta + likelihood_derivative / information_sum
                
                # Convergence check
                if abs(theta_new - theta) < 0.001:
                    break
                    
                theta = max(self.irt_params.min_theta, 
                           min(self.irt_params.max_theta, theta_new))
            else:
                break
        
        # Calculate standard error
        se = 1 / np.sqrt(max(information_sum, 0.1))
        
        return theta, se
    
    def select_next_item(self, theta, available_questions, answered_questions):
        """Select the most informative item using Maximum Information criterion"""
        if not available_questions:
            return None
        
        answered_ids = [q["question_id"] for q in answered_questions]
        candidates = [q for q in available_questions if q["question_id"] not in answered_ids]
        
        if not candidates:
            return None
        
        # Calculate information for each candidate
        best_question = None
        max_information = -1
        
        for question in candidates:
            information = self.calculate_item_information(
                theta, 
                question["difficulty"], 
                question["discrimination"]
            )
            
            if information > max_information:
                max_information = information
                best_question = question
        
        return best_question
    
    def should_stop_testing(self, se, num_questions, dimension_questions):
        """Determine if testing should stop based on CAT criteria"""
        # Stop if standard error is low enough
        if se <= self.irt_params.target_se:
            return True
        
        # Stop if maximum questions per dimension reached
        if len(dimension_questions) >= self.irt_params.max_per_dimension:
            return True
        
        # Stop if maximum total questions reached
        if num_questions >= self.irt_params.max_questions:
            return True
        
        # Ensure minimum questions per dimension
        if len(dimension_questions) < self.irt_params.min_per_dimension:
            return False
        
        return False

# Initialize global objects
question_generator = QuestionGenerator(groq_client)
adaptive_engine = AdaptiveTestEngine()

def load_sessions():  
    """Load sessions from file"""
    global sessions
    if os.path.exists(SESSIONS_FILE):
        try:
            with open(SESSIONS_FILE, 'rb') as f:
                sessions = pickle.load(f)
            print(f"Loaded {len(sessions)} sessions from file")
        except Exception as e:
            print(f"Error loading sessions: {e}")
            sessions = {}
    else:
        sessions = {}

def save_sessions():
    """Save sessions to file"""
    try:
        os.makedirs(os.path.dirname(SESSIONS_FILE), exist_ok=True)
        with open(SESSIONS_FILE, 'wb') as f:
            pickle.dump(sessions, f)
        print(f"Successfully saved {len(sessions)} sessions to file")
    except Exception as e:
        print(f"Error saving sessions: {e}")

# Initialize sessions - load from file if exists, otherwise start empty
sessions = {}
print("Initializing sessions...")
load_sessions()
print(f"Sessions initialized with {len(sessions)} existing sessions")

@app.on_event("startup")
async def startup_event():
    """Add sample data when the application starts"""
    print("Adding sample data on startup...")
    
    # Load existing sessions first
    load_sessions()
    
    # Only add sample data if no sessions exist
    if len(sessions) == 0:
        add_sample_data()
        save_sessions()
        print(f"Sample data added. Total sessions: {len(sessions)}")
    else:
        print(f"Sessions already exist: {len(sessions)}")

# Add some sample data for testing
def add_sample_data():
    print("Executing add_sample_data function...")
    sample_sessions = {
        "99d50bc7-7b2b-45d2-be07-12a7cded0229": {
            "session_id": "99d50bc7-7b2b-45d2-be07-12a7cded0229",
            "name": "Ø§Ù…ÙŠØ±Ù‡ Ø³ÙŠØ¯ Ù…Ø­Ù…Ø¯",
            "gender": "female",
            "birth_year": 2002,
            "education_level": "bachelor",
            "marital_status": "single",
            "status": "completed",
            "current_dimension": "neuroticism",
            "created_at": "2025-08-03T18:31:44.814986",
            "completed_at": "2025-08-03T18:45:22.123456",
            "questions_answered": {
                "openness": [{"question_id": "o1", "response": 3}, {"question_id": "o2", "response": 4}],
                "conscientiousness": [{"question_id": "c1", "response": 2}, {"question_id": "c2", "response": 5}],
                "extraversion": [{"question_id": "e1", "response": 3}, {"question_id": "e2", "response": 4}],
                "agreeableness": [{"question_id": "a1", "response": 4}, {"question_id": "a2", "response": 3}],
                "neuroticism": [{"question_id": "n1", "response": 2}, {"question_id": "n2", "response": 3}]
            },
            "theta": {"openness": 0.1, "conscientiousness": -0.2, "extraversion": 0.3, "agreeableness": 0.0, "neuroticism": -0.4},
            "dimension_question_count": {"openness": 10, "conscientiousness": 10, "extraversion": 10, "agreeableness": 10, "neuroticism": 10}
        },
        "e54a9ce9-04ea-4aee-aa17-9bf43b3f16d8": {
            "session_id": "e54a9ce9-04ea-4aee-aa17-9bf43b3f16d8",
            "name": "Ø§Ø­Ù…Ø¯ Ù…Ø­Ù…Ø¯ Ø¹Ù„ÙŠ",
            "gender": "male",
            "birth_year": 1995,
            "education_level": "master",
            "marital_status": "married",
            "status": "completed",
            "current_dimension": "neuroticism",
            "created_at": "2025-08-03T17:20:15.123456",
            "completed_at": "2025-08-03T17:35:45.987654",
            "questions_answered": {
                "openness": [{"question_id": "o1", "response": 4}, {"question_id": "o2", "response": 5}],
                "conscientiousness": [{"question_id": "c1", "response": 3}, {"question_id": "c2", "response": 4}],
                "extraversion": [{"question_id": "e1", "response": 5}, {"question_id": "e2", "response": 4}],
                "agreeableness": [{"question_id": "a1", "response": 3}, {"question_id": "a2", "response": 4}],
                "neuroticism": [{"question_id": "n1", "response": 2}, {"question_id": "n2", "response": 1}]
            },
            "theta": {"openness": 0.3, "conscientiousness": 0.1, "extraversion": 0.5, "agreeableness": 0.2, "neuroticism": -0.6},
            "dimension_question_count": {"openness": 10, "conscientiousness": 10, "extraversion": 10, "agreeableness": 10, "neuroticism": 10}
        },
        "b234cd56-789e-4fgh-ijkl-mnop12345678": {
            "session_id": "b234cd56-789e-4fgh-ijkl-mnop12345678",
            "name": "ÙØ§Ø·Ù…Ø© Ø§Ø­Ù…Ø¯",
            "gender": "female",
            "birth_year": 1988,
            "education_level": "high_school",
            "marital_status": "divorced",
            "status": "active",
            "current_dimension": "extraversion",
            "created_at": "2025-08-03T19:10:30.456789",
            "completed_at": None,
            "questions_answered": {
                "openness": [{"question_id": "o1", "response": 2}, {"question_id": "o2", "response": 3}],
                "conscientiousness": [{"question_id": "c1", "response": 4}, {"question_id": "c2", "response": 3}],
                "extraversion": [{"question_id": "e1", "response": 3}],
                "agreeableness": [],
                "neuroticism": []
            },
            "theta": {"openness": -0.1, "conscientiousness": 0.2, "extraversion": 0.0, "agreeableness": 0.0, "neuroticism": 0.0},
            "dimension_question_count": {"openness": 10, "conscientiousness": 10, "extraversion": 5, "agreeableness": 0, "neuroticism": 0}
        }
    }
    sessions.update(sample_sessions)
    print(f"Sample sessions added: {len(sample_sessions)} sessions")

questions_db = {
    "openness": [
        {"question_id": "o1", "text": "Ø£Ø³ØªÙ…ØªØ¹ Ø¨Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ù…Ø¬Ø±Ø¯Ø© ÙˆØ§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù†Ø¸Ø±ÙŠØ©.", "reverse_scored": False, "difficulty": -1},
        {"question_id": "o2", "text": "Ù„Ø¯ÙŠ Ø®ÙŠØ§Ù„ Ø®ØµØ¨ Ø¬Ø¯Ø§Ù‹.", "reverse_scored": False, "difficulty": -0.5},
        {"question_id": "o3", "text": "Ø£Ù†Ø§ ÙØ¶ÙˆÙ„ÙŠ Ø¨Ø´Ø£Ù† ÙƒÙ„ Ø´ÙŠØ¡ ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹.", "reverse_scored": False, "difficulty": 0},
        {"question_id": "o4", "text": "Ø£ÙØ¶Ù„ Ø§Ù„Ø±ÙˆØªÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„ØªØºÙŠÙŠØ±.", "reverse_scored": True, "difficulty": 0.5},
        {"question_id": "o5", "text": "Ø£Ù†Ø§ Ù…Ø¨Ø¯Ø¹ ÙˆØ£Ø­Ø¨ Ø§Ø¨ØªÙƒØ§Ø± Ø£Ø´ÙŠØ§Ø¡ Ø¬Ø¯ÙŠØ¯Ø©.", "reverse_scored": False, "difficulty": 1},
        {"question_id": "o6", "text": "Ø£Ø¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ ÙÙ‡Ù… Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ù…Ø¬Ø±Ø¯Ø©.", "reverse_scored": True, "difficulty": -1},
        {"question_id": "o7", "text": "Ø£Ø­Ø¨ ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©.", "reverse_scored": False, "difficulty": -0.5},
        {"question_id": "o8", "text": "Ù„Ø³Øª Ù…Ù‡ØªÙ…Ø§Ù‹ Ø¨Ø§Ù„ÙÙ†ÙˆÙ†.", "reverse_scored": True, "difficulty": 0},
        {"question_id": "o9", "text": "Ø£Ø­Ø¨ Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©.", "reverse_scored": False, "difficulty": 0.5},
        {"question_id": "o10", "text": "Ø£Ù…ÙŠÙ„ Ø¥Ù„Ù‰ Ø§Ù„ØªØµÙˆÙŠØª Ù„Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ø§Ù„Ù…Ø­Ø§ÙØ¸ÙŠÙ†.", "reverse_scored": True, "difficulty": 1}
    ],
    "conscientiousness": [
        {"question_id": "c1", "text": "Ø£Ù†Ø§ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…Ø³ØªØ¹Ø¯ ÙˆÙ…Ù†Ø¸Ù….", "reverse_scored": False, "difficulty": -1},
        {"question_id": "c2", "text": "Ø£ØªØ±Ùƒ Ø£Ø´ÙŠØ§Ø¦ÙŠ ÙÙˆØ¶ÙˆÙŠØ§Ù‹.", "reverse_scored": True, "difficulty": -0.5},
        {"question_id": "c3", "text": "Ø£Ù‡ØªÙ… Ø¨Ø§Ù„ØªÙØ§ØµÙŠÙ„.", "reverse_scored": False, "difficulty": 0},
        {"question_id": "c4", "text": "Ø£Ø¤Ø¬Ù„ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ù‡Ù…Ø©.", "reverse_scored": True, "difficulty": 0.5},
        {"question_id": "c5", "text": "Ø£ØªØ¨Ø¹ Ø¬Ø¯ÙˆÙ„Ø§Ù‹ Ø²Ù…Ù†ÙŠØ§Ù‹.", "reverse_scored": False, "difficulty": 1},
        {"question_id": "c6", "text": "Ø£Ù†Ø§ Ø¯Ù‚ÙŠÙ‚ ÙÙŠ Ø¹Ù…Ù„ÙŠ.", "reverse_scored": False, "difficulty": -1},
        {"question_id": "c7", "text": "Ø£Ù†Ø³Ù‰ Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø¥Ù„Ù‰ Ù…ÙƒØ§Ù†Ù‡Ø§ Ø§Ù„ØµØ­ÙŠØ­.", "reverse_scored": True, "difficulty": -0.5},
        {"question_id": "c8", "text": "Ø£Ø­Ø¨ Ø§Ù„Ù†Ø¸Ø§Ù….", "reverse_scored": False, "difficulty": 0},
        {"question_id": "c9", "text": "Ø£Ø¬Ø¯ ØµØ¹ÙˆØ¨Ø© ÙÙŠ Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„Ø®Ø·Ø·.", "reverse_scored": True, "difficulty": 0.5},
        {"question_id": "c10", "text": "Ø£Ù†Ø§ Ù…Ø¬ØªÙ‡Ø¯ ÙˆÙ…Ø«Ø§Ø¨Ø±.", "reverse_scored": False, "difficulty": 1}
    ],
    "extraversion": [
        {"question_id": "e1", "text": "Ø£Ù†Ø§ Ù…Ø­ÙˆØ± Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… ÙÙŠ Ø§Ù„Ø­ÙÙ„Ø§Øª.", "reverse_scored": False, "difficulty": -1},
        {"question_id": "e2", "text": "Ù„Ø§ Ø£ØªØ­Ø¯Ø« ÙƒØ«ÙŠØ±Ø§Ù‹.", "reverse_scored": True, "difficulty": -0.5},
        {"question_id": "e3", "text": "Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø±Ø§Ø­Ø© Ø­ÙˆÙ„ Ø§Ù„Ù†Ø§Ø³.", "reverse_scored": False, "difficulty": 0},
        {"question_id": "e4", "text": "Ø£ÙØ¶Ù„ Ø§Ù„Ø¨Ù‚Ø§Ø¡ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©.", "reverse_scored": True, "difficulty": 0.5},
        {"question_id": "e5", "text": "Ø£Ø¨Ø¯Ø£ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª.", "reverse_scored": False, "difficulty": 1},
        {"question_id": "e6", "text": "Ù„Ø¯ÙŠ Ø¯Ø§Ø¦Ø±Ø© ÙˆØ§Ø³Ø¹Ø© Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ø±Ù.", "reverse_scored": False, "difficulty": -1},
        {"question_id": "e7", "text": "Ø£Ù†Ø§ Ù‡Ø§Ø¯Ø¦ Ø­ÙˆÙ„ Ø§Ù„ØºØ±Ø¨Ø§Ø¡.", "reverse_scored": True, "difficulty": -0.5},
        {"question_id": "e8", "text": "Ù„Ø§ Ø£Ù…Ø§Ù†Ø¹ Ø£Ù† Ø£ÙƒÙˆÙ† Ù…Ø±ÙƒØ² Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù….", "reverse_scored": False, "difficulty": 0},
        {"question_id": "e9", "text": "Ø£ÙØ¶Ù„ Ù‚Ø¶Ø§Ø¡ Ø§Ù„ÙˆÙ‚Øª Ø¨Ù…ÙØ±Ø¯ÙŠ.", "reverse_scored": True, "difficulty": 0.5},
        {"question_id": "e10", "text": "Ø£Ù†Ø§ Ù…ÙØ¹Ù… Ø¨Ø§Ù„Ø­ÙŠÙˆÙŠØ© ÙˆØ§Ù„Ù†Ø´Ø§Ø·.", "reverse_scored": False, "difficulty": 1}
    ],
    "agreeableness": [
        {"question_id": "a1", "text": "Ø£ØªØ¹Ø§Ø·Ù Ù…Ø¹ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†.", "reverse_scored": False, "difficulty": -1},
        {"question_id": "a2", "text": "Ù„Ø³Øª Ù…Ù‡ØªÙ…Ø§Ù‹ Ø¨Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†.", "reverse_scored": True, "difficulty": -0.5},
        {"question_id": "a3", "text": "Ù„Ø¯ÙŠ Ù‚Ù„Ø¨ Ø­Ù†ÙˆÙ†.", "reverse_scored": False, "difficulty": 0},
        {"question_id": "a4", "text": "Ø£Ù‡ÙŠÙ† Ø§Ù„Ù†Ø§Ø³.", "reverse_scored": True, "difficulty": 0.5},
        {"question_id": "a5", "text": "Ø£Ø¬Ø¹Ù„ Ø§Ù„Ù†Ø§Ø³ ÙŠØ´Ø¹Ø±ÙˆÙ† Ø¨Ø§Ù„Ø±Ø§Ø­Ø©.", "reverse_scored": False, "difficulty": 1},
        {"question_id": "a6", "text": "Ø£Ù†Ø§ ØµØ¨ÙˆØ± Ù…Ø¹ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†.", "reverse_scored": False, "difficulty": -1},
        {"question_id": "a7", "text": "Ø£Ù†Ø§ Ø³Ø±ÙŠØ¹ Ø§Ù„ØºØ¶Ø¨.", "reverse_scored": True, "difficulty": -0.5},
        {"question_id": "a8", "text": "Ø£Ø«Ù‚ Ø¨Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†.", "reverse_scored": False, "difficulty": 0},
        {"question_id": "a9", "text": "Ø£Ù†Ø§ Ù…ØªØ´ÙƒÙƒ ÙÙŠ Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†.", "reverse_scored": True, "difficulty": 0.5},
        {"question_id": "a10", "text": "Ø£Ù†Ø§ Ù…ØªØ¹Ø§ÙˆÙ† Ø¨Ø·Ø¨Ø¹ÙŠ.", "reverse_scored": False, "difficulty": 1}
    ],
    "neuroticism": [
        {"question_id": "n1", "text": "Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„ØªÙˆØªØ± Ø¨Ø³Ù‡ÙˆÙ„Ø©.", "reverse_scored": False, "difficulty": -1},
        {"question_id": "n2", "text": "Ø£Ù†Ø§ Ù…Ø³ØªØ±Ø®Ù ÙÙŠ Ù…Ø¹Ø¸Ù… Ø§Ù„Ø£ÙˆÙ‚Ø§Øª.", "reverse_scored": True, "difficulty": -0.5},
        {"question_id": "n3", "text": "Ø£Ù‚Ù„Ù‚ Ø¨Ø´Ø£Ù† Ø§Ù„Ø£Ø´ÙŠØ§Ø¡.", "reverse_scored": False, "difficulty": 0},
        {"question_id": "n4", "text": "Ù†Ø§Ø¯Ø±Ø§Ù‹ Ù…Ø§ Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ø­Ø²Ù†.", "reverse_scored": True, "difficulty": 0.5},
        {"question_id": "n5", "text": "Ø£Ù†Ø§ Ù…ØªÙ‚Ù„Ø¨ Ø§Ù„Ù…Ø²Ø§Ø¬.", "reverse_scored": False, "difficulty": 1},
        {"question_id": "n6", "text": "Ø£ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªÙˆØªØ± Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯.", "reverse_scored": True, "difficulty": -1},
        {"question_id": "n7", "text": "Ø£Ø´Ø¹Ø± Ø¨Ø§Ù„Ù‚Ù„Ù‚ ÙƒØ«ÙŠØ±Ø§Ù‹.", "reverse_scored": False, "difficulty": -0.5},
        {"question_id": "n8", "text": "Ø£Ù†Ø§ Ù…Ø³ØªÙ‚Ø± Ø¹Ø§Ø·ÙÙŠØ§Ù‹.", "reverse_scored": True, "difficulty": 0},
        {"question_id": "n9", "text": "ÙŠÙ…ÙƒÙ† Ø£Ù† Ø£ÙƒÙˆÙ† Ø³Ø±ÙŠØ¹ Ø§Ù„Ø§Ù†ÙØ¹Ø§Ù„.", "reverse_scored": False, "difficulty": 0.5},
        {"question_id": "n10", "text": "Ø£Ù†Ø§ Ø±Ø§Ø¶Ù Ø¹Ù† Ù†ÙØ³ÙŠ.", "reverse_scored": True, "difficulty": 1}
    ]
}

# Pydantic models
class SessionCreate(BaseModel):
    name: str
    gender: Optional[str] = None
    birth_year: Optional[int] = None
    birthYear: Optional[int] = None  # Accept both formats
    education_level: Optional[str] = None
    educationLevel: Optional[str] = None  # Accept both formats
    marital_status: Optional[str] = None
    maritalStatus: Optional[str] = None  # Accept both formats

class SessionResponse(BaseModel):
    session_id: str
    name: str
    status: str
    current_dimension: str
    current_question_number: int
    total_dimensions: int
    dimension_progress: Dict[str, int]

class Question(BaseModel):
    question_id: str
    text: str
    dimension: str
    question_number: int
    reverse_scored: bool = False
    total_answered: int = 0
    total_questions: int = 50  # Updated from 200 to 50
    progress_percentage: float = 0.0

class AnswerSubmission(BaseModel):
    session_id: str
    question_id: str
    response: int

class AdminLoginRequest(BaseModel):
    username: str
    password: str

class AdminLoginResponse(BaseModel):
    success: bool
    token: str = None
    message: str = None

@app.post("/api/sessions", response_model=SessionResponse)
async def create_session(session_data: SessionCreate):
    try:
        session_id = str(uuid.uuid4())
        
        # Handle both camelCase and snake_case field names
        birth_year = session_data.birth_year or session_data.birthYear
        education_level = session_data.education_level or session_data.educationLevel
        marital_status = session_data.marital_status or session_data.maritalStatus
        
        # Create demographic profile for question generation
        demographics = {
            "gender": session_data.gender,
            "birth_year": birth_year,
            "education_level": education_level,
            "marital_status": marital_status,
            "age_group": _calculate_age_group(birth_year)
        }
        
        # Generate personalized questions for all dimensions
        print(f"Generating personalized questions for session {session_id}...")
        all_questions = {}
        
        for dimension in PERSONALITY_DIMENSIONS:
            try:
                questions = await question_generator.generate_personalized_questions(
                    demographics, dimension, count=10  # Generate 10 questions per dimension (50 total)
                )
                all_questions[dimension] = questions
                print(f"Generated {len(questions)} questions for {dimension}")
            except Exception as e:
                print(f"Error generating questions for {dimension}: {e}")
                # Use fallback questions
                all_questions[dimension] = question_generator._get_optimized_question_bank(
                    dimension, demographics, 10  # Use 10 questions per dimension
                )
        
        # Create session with generated questions
        sessions[session_id] = {
            "session_id": session_id,
            "name": session_data.name,
            "gender": session_data.gender,
            "birth_year": birth_year,
            "education_level": education_level,
            "marital_status": marital_status,
            "demographics": demographics,
            "status": "active",
            "current_dimension": "openness",
            "created_at": datetime.now().isoformat(),
            "completed_at": None,
            "questions_answered": {dim: [] for dim in PERSONALITY_DIMENSIONS},
            "theta": {dim: 0.0 for dim in PERSONALITY_DIMENSIONS},
            "se": {dim: 1.0 for dim in PERSONALITY_DIMENSIONS},
            "dimension_question_count": {dim: 0 for dim in PERSONALITY_DIMENSIONS},
            "generated_questions": all_questions,
            "total_questions_answered": 0
        }
        
        # Save sessions to file
        save_sessions()
        
        print(f"DEBUG: New session created: {session_id}. Total sessions now: {len(sessions)}")
        
        return SessionResponse(
            session_id=session_id,
            name=session_data.name,
            status="active",
            current_dimension="openness",
            current_question_number=1,
            total_dimensions=5,
            dimension_progress={dim: 0 for dim in PERSONALITY_DIMENSIONS}
        )
    except Exception as e:
        print(f"Error creating session: {e}")
        raise HTTPException(status_code=500, detail=f"Error creating session: {str(e)}")

def _calculate_age_group(birth_year):
    """Calculate age group from birth year"""
    if not birth_year:
        return "unknown"
    
    try:
        age = 2025 - int(birth_year)
        if age < 20:
            return "teen"
        elif age < 30:
            return "young_adult"
        elif age < 50:
            return "middle_age"
        else:
            return "senior"
    except:
        return "unknown"

@app.get("/api/sessions/{session_id}/question", response_model=Question)
async def get_current_question(session_id: str):
    try:
        if session_id not in sessions:
            raise HTTPException(status_code=404, detail="Session not found")
        
        session = sessions[session_id]
        current_dimension = session["current_dimension"]
        
        if session["dimension_question_count"][current_dimension] >= adaptive_engine.irt_params.max_per_dimension:
            # Move to next dimension after reaching max questions per dimension (5 dimensions Ã— 10 = 50 total)
            dimensions = list(questions_db.keys())
            current_index = dimensions.index(current_dimension)
            if current_index + 1 < len(dimensions):
                session["current_dimension"] = dimensions[current_index + 1]
                current_dimension = session["current_dimension"]
            else:
                # Test is complete
                session["status"] = "completed"
                raise HTTPException(status_code=404, detail="No more questions")

        # Adaptive question selection
        theta = session["theta"][current_dimension]
        answered_ids = [q["question_id"] for q in session["questions_answered"][current_dimension]]
        
        # Use generated questions from session instead of questions_db
        available_questions = [q for q in session["generated_questions"][current_dimension] if q["question_id"] not in answered_ids]
        
        # Select question based on whether it's the first question or not
        if len(answered_ids) == 0:
            # For the first question in each dimension, select randomly from medium difficulty questions
            import random
            medium_questions = [q for q in available_questions if -0.5 <= q["difficulty"] <= 0.5]
            if medium_questions:
                next_question = random.choice(medium_questions)
            else:
                next_question = random.choice(available_questions)
        else:
            # For subsequent questions, use adaptive IRT selection
            next_question = min(available_questions, key=lambda q: abs(q["difficulty"] - theta))
        
        # Get first name from full name for personalization
        first_name = session["name"].split()[0] if session["name"] else ""
        
        # Smart gender detection and question personalization
        def personalize_question(text, name, gender=None):
            if not name:
                return text
            
            # Detect gender from name or use provided gender
            is_female = False
            if gender == "female":
                is_female = True
            elif gender == "male":
                is_female = False
            else:
                # Try to detect from name endings (simple heuristic)
                female_endings = ['Ø©', 'Ø§Ø¡', 'Ù‰', 'Ø§Ù†', 'ÙŠÙ†']
                is_female = any(name.endswith(ending) for ending in female_endings)
            
            # Convert statement to question format
            question_text = text
            
            # Replace "Ø£Ù†Ø§" with "Ø£Ù†Øª/Ø£Ù†ØªÙ" and adjust for gender
            if "Ø£Ù†Ø§ " in question_text:
                if is_female:
                    question_text = question_text.replace("Ø£Ù†Ø§ ", "Ø£Ù†ØªÙ ")
                else:
                    question_text = question_text.replace("Ø£Ù†Ø§ ", "Ø£Ù†Øª ")
            
            # Handle verbs - convert to second person with gender agreement
            if is_female:
                # Female second person verbs
                question_text = question_text.replace("Ø£Ø³ØªÙ…ØªØ¹", "ØªØ³ØªÙ…ØªØ¹ÙŠÙ†")
                question_text = question_text.replace("Ø£Ø­Ø¨", "ØªØ­Ø¨ÙŠÙ†") 
                question_text = question_text.replace("Ø£Ù‡ØªÙ…", "ØªÙ‡ØªÙ…ÙŠÙ†")
                question_text = question_text.replace("Ø£ØªØ¨Ø¹", "ØªØªØ¨Ø¹ÙŠÙ†")
                question_text = question_text.replace("Ø£Ø¤Ø¬Ù„", "ØªØ¤Ø¬Ù„ÙŠÙ†")
                question_text = question_text.replace("Ø£Ù†Ø³Ù‰", "ØªÙ†Ø³ÙŠÙ†")
                question_text = question_text.replace("Ø£Ø¬Ø¯", "ØªØ¬Ø¯ÙŠÙ†")
                question_text = question_text.replace("Ø£Ø´Ø¹Ø±", "ØªØ´Ø¹Ø±ÙŠÙ†")
                question_text = question_text.replace("Ø£ÙØ¶Ù„", "ØªÙØ¶Ù„ÙŠÙ†")
                question_text = question_text.replace("Ø£Ø¨Ø¯Ø£", "ØªØ¨Ø¯Ø¦ÙŠÙ†")
                question_text = question_text.replace("Ø£ØªØ¹Ø§Ø·Ù", "ØªØªØ¹Ø§Ø·ÙÙŠÙ†")
                question_text = question_text.replace("Ø£Ø¬Ø¹Ù„", "ØªØ¬Ø¹Ù„ÙŠÙ†")
                question_text = question_text.replace("Ø£Ù‡ÙŠÙ†", "ØªÙ‡ÙŠÙ†ÙŠÙ†")
                question_text = question_text.replace("Ø£Ø«Ù‚", "ØªØ«Ù‚ÙŠÙ†")
                question_text = question_text.replace("Ø£Ù‚Ù„Ù‚", "ØªÙ‚Ù„Ù‚ÙŠÙ†")
                question_text = question_text.replace("Ø£ØªØ¹Ø§Ù…Ù„", "ØªØªØ¹Ø§Ù…Ù„ÙŠÙ†")
                question_text = question_text.replace("Ø£Ù…ÙŠÙ„", "ØªÙ…ÙŠÙ„ÙŠÙ†")
                question_text = question_text.replace("Ø£ØªØ±Ùƒ", "ØªØªØ±ÙƒÙŠÙ†")
                question_text = question_text.replace("Ø£ØªØ­Ø¯Ø«", "ØªØªØ­Ø¯Ø«ÙŠÙ†")
                question_text = question_text.replace("Ø£Ù…Ø§Ù†Ø¹", "ØªÙ…Ø§Ù†Ø¹ÙŠÙ†")
                
                # Adjust adjectives and descriptions for feminine
                question_text = question_text.replace("ÙØ¶ÙˆÙ„ÙŠ", "ÙØ¶ÙˆÙ„ÙŠØ©")
                question_text = question_text.replace("Ù…Ø³ØªØ¹Ø¯", "Ù…Ø³ØªØ¹Ø¯Ø©")
                question_text = question_text.replace("Ù…Ù†Ø¸Ù…", "Ù…Ù†Ø¸Ù…Ø©")  
                question_text = question_text.replace("Ø¯Ù‚ÙŠÙ‚", "Ø¯Ù‚ÙŠÙ‚Ø©")
                question_text = question_text.replace("Ù…Ø¬ØªÙ‡Ø¯", "Ù…Ø¬ØªÙ‡Ø¯Ø©")
                question_text = question_text.replace("Ù…Ø«Ø§Ø¨Ø±", "Ù…Ø«Ø§Ø¨Ø±Ø©")
                question_text = question_text.replace("Ù…Ø¨Ø¯Ø¹", "Ù…Ø¨Ø¯Ø¹Ø©")
                question_text = question_text.replace("Ù‡Ø§Ø¯Ø¦", "Ù‡Ø§Ø¯Ø¦Ø©")
                question_text = question_text.replace("ØµØ¨ÙˆØ±", "ØµØ¨ÙˆØ±Ø©")
                question_text = question_text.replace("Ù…ØªØ¹Ø§ÙˆÙ†", "Ù…ØªØ¹Ø§ÙˆÙ†Ø©")
                question_text = question_text.replace("Ù…Ø³ØªØ±Ø®", "Ù…Ø³ØªØ±Ø®ÙŠØ©")
                question_text = question_text.replace("Ù…Ø³ØªØ±Ø®ÙŠØ©Ù", "Ù…Ø³ØªØ±Ø®ÙŠØ©")  # Fix tanween
                question_text = question_text.replace("Ù…ØªÙ‚Ù„Ø¨", "Ù…ØªÙ‚Ù„Ø¨Ø©")
                question_text = question_text.replace("Ù…Ø³ØªÙ‚Ø±", "Ù…Ø³ØªÙ‚Ø±Ø©")
                question_text = question_text.replace("Ø±Ø§Ø¶", "Ø±Ø§Ø¶ÙŠØ©")
                question_text = question_text.replace("Ø±Ø§Ø¶ÙŠØ©Ù", "Ø±Ø§Ø¶ÙŠØ©")  # Fix tanween
                question_text = question_text.replace("Ù…ÙØ¹Ù…", "Ù…ÙØ¹Ù…Ø©")
                question_text = question_text.replace("Ø³Ø±ÙŠØ¹", "Ø³Ø±ÙŠØ¹Ø©")
                
                # Handle "Ù„Ø¯ÙŠ" (I have)
                question_text = question_text.replace("Ù„Ø¯ÙŠ", "Ù„Ø¯ÙŠÙƒÙ")
                
                # Handle "ÙÙŠ Ø¹Ù…Ù„ÙŠ" (in my work)
                question_text = question_text.replace("ÙÙŠ Ø¹Ù…Ù„ÙŠ", "ÙÙŠ Ø¹Ù…Ù„ÙƒÙ")
                
                # Handle "Ø¨Ù…ÙØ±Ø¯ÙŠ" (alone)
                question_text = question_text.replace("Ø¨Ù…ÙØ±Ø¯ÙŠ", "Ø¨Ù…ÙØ±Ø¯ÙƒÙ")
                
                # Handle "Ø¨Ø·Ø¨Ø¹ÙŠ" (by nature)
                question_text = question_text.replace("Ø¨Ø·Ø¨Ø¹ÙŠ", "Ø¨Ø·Ø¨Ø¹ÙƒÙ")
                
                # Handle "Ø¹Ù† Ù†ÙØ³ÙŠ" (about myself)
                question_text = question_text.replace("Ø¹Ù† Ù†ÙØ³ÙŠ", "Ø¹Ù† Ù†ÙØ³ÙƒÙ")
                
                # Handle negations like "Ù„Ø³Øª"
                question_text = question_text.replace("Ù„Ø³Øª Ù…Ù‡ØªÙ…Ø§Ù‹", "Ù„Ø³ØªÙ Ù…Ù‡ØªÙ…Ø©")
                question_text = question_text.replace("Ù„Ø³ØªÙÙ", "Ù„Ø³ØªÙ")  # Fix double kasra
                question_text = question_text.replace("Ù„Ø³Øª", "Ù„Ø³ØªÙ")
                
                # Handle "Ù„Ø§" negations
                question_text = question_text.replace("Ù„Ø§ Ø£ØªØ­Ø¯Ø«", "Ù„Ø§ ØªØªØ­Ø¯Ø«ÙŠÙ†")
                question_text = question_text.replace("Ù„Ø§ Ø£Ù…Ø§Ù†Ø¹", "Ù„Ø§ ØªÙ…Ø§Ù†Ø¹ÙŠÙ†")
                
                # Handle "ÙŠÙ…ÙƒÙ† Ø£Ù† Ø£ÙƒÙˆÙ†" 
                question_text = question_text.replace("ÙŠÙ…ÙƒÙ† Ø£Ù† Ø£ÙƒÙˆÙ†", "ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ†ÙŠ")
                
                # Handle other possession forms
                question_text = question_text.replace("Ø£Ø´ÙŠØ§Ø¦ÙŠ", "Ø£Ø´ÙŠØ§Ø¡ÙƒÙ")
                
            else:
                # Male second person verbs
                question_text = question_text.replace("Ø£Ø³ØªÙ…ØªØ¹", "ØªØ³ØªÙ…ØªØ¹")
                question_text = question_text.replace("Ø£Ø­Ø¨", "ØªØ­Ø¨")
                question_text = question_text.replace("Ø£Ù‡ØªÙ…", "ØªÙ‡ØªÙ…") 
                question_text = question_text.replace("Ø£ØªØ¨Ø¹", "ØªØªØ¨Ø¹")
                question_text = question_text.replace("Ø£Ø¤Ø¬Ù„", "ØªØ¤Ø¬Ù„")
                question_text = question_text.replace("Ø£Ù†Ø³Ù‰", "ØªÙ†Ø³Ù‰")
                question_text = question_text.replace("Ø£Ø¬Ø¯", "ØªØ¬Ø¯")
                question_text = question_text.replace("Ø£Ø´Ø¹Ø±", "ØªØ´Ø¹Ø±")
                question_text = question_text.replace("Ø£ÙØ¶Ù„", "ØªÙØ¶Ù„")
                question_text = question_text.replace("Ø£Ø¨Ø¯Ø£", "ØªØ¨Ø¯Ø£")
                question_text = question_text.replace("Ø£ØªØ¹Ø§Ø·Ù", "ØªØªØ¹Ø§Ø·Ù")
                question_text = question_text.replace("Ø£Ø¬Ø¹Ù„", "ØªØ¬Ø¹Ù„")
                question_text = question_text.replace("Ø£Ù‡ÙŠÙ†", "ØªÙ‡ÙŠÙ†")
                question_text = question_text.replace("Ø£Ø«Ù‚", "ØªØ«Ù‚")
                question_text = question_text.replace("Ø£Ù‚Ù„Ù‚", "ØªÙ‚Ù„Ù‚")
                question_text = question_text.replace("Ø£ØªØ¹Ø§Ù…Ù„", "ØªØªØ¹Ø§Ù…Ù„")
                question_text = question_text.replace("Ø£Ù…ÙŠÙ„", "ØªÙ…ÙŠÙ„")
                question_text = question_text.replace("Ø£ØªØ±Ùƒ", "ØªØªØ±Ùƒ")
                question_text = question_text.replace("Ø£ØªØ­Ø¯Ø«", "ØªØªØ­Ø¯Ø«")
                question_text = question_text.replace("Ø£Ù…Ø§Ù†Ø¹", "ØªÙ…Ø§Ù†Ø¹")
                
                # Handle "Ù„Ø¯ÙŠ" (I have)
                question_text = question_text.replace("Ù„Ø¯ÙŠ", "Ù„Ø¯ÙŠÙƒ")
                
                # Handle "ÙÙŠ Ø¹Ù…Ù„ÙŠ" (in my work)
                question_text = question_text.replace("ÙÙŠ Ø¹Ù…Ù„ÙŠ", "ÙÙŠ Ø¹Ù…Ù„Ùƒ")
                
                # Handle "Ø¨Ù…ÙØ±Ø¯ÙŠ" (alone)
                question_text = question_text.replace("Ø¨Ù…ÙØ±Ø¯ÙŠ", "Ø¨Ù…ÙØ±Ø¯Ùƒ")
                
                # Handle "Ø¨Ø·Ø¨Ø¹ÙŠ" (by nature)
                question_text = question_text.replace("Ø¨Ø·Ø¨Ø¹ÙŠ", "Ø¨Ø·Ø¨Ø¹Ùƒ")
                
                # Handle "Ø¹Ù† Ù†ÙØ³ÙŠ" (about myself)
                question_text = question_text.replace("Ø¹Ù† Ù†ÙØ³ÙŠ", "Ø¹Ù† Ù†ÙØ³Ùƒ")
                
                # Handle negations like "Ù„Ø³Øª"
                question_text = question_text.replace("Ù„Ø³Øª", "Ù„Ø³Øª")
                
                # Handle "Ù„Ø§" negations
                question_text = question_text.replace("Ù„Ø§ Ø£ØªØ­Ø¯Ø«", "Ù„Ø§ ØªØªØ­Ø¯Ø«")
                question_text = question_text.replace("Ù„Ø§ Ø£Ù…Ø§Ù†Ø¹", "Ù„Ø§ ØªÙ…Ø§Ù†Ø¹")
                
                # Handle "ÙŠÙ…ÙƒÙ† Ø£Ù† Ø£ÙƒÙˆÙ†" 
                question_text = question_text.replace("ÙŠÙ…ÙƒÙ† Ø£Ù† Ø£ÙƒÙˆÙ†", "ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ†")
                
                # Handle other possession forms
                question_text = question_text.replace("Ø£Ø´ÙŠØ§Ø¦ÙŠ", "Ø£Ø´ÙŠØ§Ø¡Ùƒ")
            
            # Add question format and name
            if question_text.endswith('.'):
                question_text = question_text[:-1]
            
            # Check if question already starts with "Ù‡Ù„"
            if question_text.strip().startswith("Ù‡Ù„"):
                return f"{question_text} ÙŠØ§ {name}ØŸ"
            else:
                return f"Ù‡Ù„ {question_text} ÙŠØ§ {name}ØŸ"
        
        # Personalize the question text
        personalized_text = personalize_question(next_question['text'], first_name, session.get('gender'))
        
        # Calculate total questions answered so far
        total_answered = sum(session["dimension_question_count"].values())
        
        return Question(
            question_id=next_question["question_id"],
            text=personalized_text,
            dimension=current_dimension,
            question_number=session["dimension_question_count"][current_dimension] + 1,
            reverse_scored=next_question["reverse_scored"],
            total_answered=total_answered,
            total_questions=adaptive_engine.irt_params.max_questions,
            progress_percentage=round((total_answered / adaptive_engine.irt_params.max_questions) * 100, 1)
        )
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error getting question: {e}")
        raise HTTPException(status_code=500, detail=f"Error getting question: {str(e)}")

@app.post("/api/answers")
async def submit_answer(answer: AnswerSubmission):
    print(f"ðŸ” Received answer submission: {answer}")
    try:
        if answer.session_id not in sessions:
            print(f"âŒ Session not found: {answer.session_id}")
            print(f"Available sessions: {list(sessions.keys())}")
            raise HTTPException(status_code=404, detail="Session not found")
        
        session = sessions[answer.session_id]
        current_dimension = session["current_dimension"]
        
        print(f"ðŸ“ Processing answer for session {answer.session_id}, dimension: {current_dimension}")
        
        # Find the question in generated questions
        question = None
        for q in session["generated_questions"][current_dimension]:
            if q["question_id"] == answer.question_id:
                question = q
                break
        
        if not question:
            print(f"âŒ Question not found: {answer.question_id}")
            print(f"Available questions in {current_dimension}: {[q['question_id'] for q in session['generated_questions'][current_dimension][:5]]}")
            raise HTTPException(status_code=404, detail="Question not found")

        # Record the answer with question details
        answered_question = {
            "question_id": answer.question_id,
            "response": answer.response,
            "difficulty": question["difficulty"],
            "discrimination": question["discrimination"],
            "reverse_scored": question.get("reverse_scored", False)
        }
        
        session["questions_answered"][current_dimension].append(answered_question)
        
        # Update dimension question count
        session["dimension_question_count"][current_dimension] = len(session["questions_answered"][current_dimension])
        
        # Update ability estimate using IRT
        answered_questions = session["questions_answered"][current_dimension]
        
        # Extract response data for theta estimation
        responses = []
        difficulties = []
        discriminations = []
        
        for aq in answered_questions:
            response_value = aq["response"]
            if aq.get("reverse_scored", False):
                response_value = 6 - response_value  # Reverse score if needed
            
            # Convert to binary response (1 if >= 4, 0 otherwise)
            responses.append(1 if response_value >= 4 else 0)
            difficulties.append(aq["difficulty"])
            discriminations.append(aq["discrimination"])
        
        # Estimate new theta and standard error
        new_theta, new_se = adaptive_engine.estimate_theta(responses, difficulties, discriminations)
        
        # Update session
        session["theta"][current_dimension] = new_theta
        session["se"][current_dimension] = new_se
        session["total_questions_answered"] = session.get("total_questions_answered", 0) + 1
        
        # Save sessions after each answer
        save_sessions()
        
        # Check if test is complete
        total_answered = session["total_questions_answered"]
        
        # Check completion criteria
        all_dimensions_complete = True
        for dim in PERSONALITY_DIMENSIONS:
            dim_questions = session["questions_answered"][dim]
            dim_se = session["se"][dim]
            
            # Check if dimension needs more questions (minimum 5, maximum 10 per dimension)
            if (len(dim_questions) < adaptive_engine.irt_params.min_per_dimension or 
                (dim_se > adaptive_engine.irt_params.target_se and len(dim_questions) < adaptive_engine.irt_params.max_per_dimension)):
                all_dimensions_complete = False
                break
        
        if (total_answered >= adaptive_engine.irt_params.max_questions or all_dimensions_complete):
            session["status"] = "completed"
            session["completed_at"] = datetime.now().isoformat()
            save_sessions()
            
            print(f"Test completed! Session {answer.session_id} status updated.")
        
        return {
            "message": "Answer submitted successfully", 
            "status": session["status"],
            "total_answered": total_answered,
            "progress_percentage": round((total_answered / adaptive_engine.irt_params.max_questions) * 100, 1),
            "current_theta": round(new_theta, 2),
            "current_se": round(new_se, 3)
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error submitting answer: {str(e)}")

def generate_comprehensive_analysis(scores, session):
    """Generate detailed personality analysis using Groq AI based on actual results"""
    
    # Get demographic info
    age = session.get("age", 25)
    gender = session.get("gender", "male")
    marital_status = session.get("marital_status", "single")
    education_level = session.get("education_level", "bachelor")
    name = session.get("name", "")
    
    # Convert scores to percentages (scores are on 1-5 scale, convert to 0-100%)
    openness_pct = ((scores["openness"] - 1) / 4) * 100
    conscientiousness_pct = ((scores["conscientiousness"] - 1) / 4) * 100  
    extraversion_pct = ((scores["extraversion"] - 1) / 4) * 100
    agreeableness_pct = ((scores["agreeableness"] - 1) / 4) * 100
    neuroticism_pct = ((scores["neuroticism"] - 1) / 4) * 100
    
    # Try to use Groq AI for detailed analysis
    if groq_client:
        try:
            # Prepare personality profile for AI
            personality_prompt = f"""
            Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù…ØªØ®ØµØµ ÙÙŠ Ø¹Ù„Ù… Ø§Ù„Ù†ÙØ³ ÙˆØ§Ù„Ø´Ø®ØµÙŠØ©. Ù‚Ù… Ø¨ÙƒØªØ§Ø¨Ø© ØªØ­Ù„ÙŠÙ„ Ø´Ø®ØµÙŠØ© Ø´Ø§Ù…Ù„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø· Ù„Ø§ ØºÙŠØ±.

            **Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®Øµ:**
            - Ø§Ù„Ø§Ø³Ù…: {name}
            - Ø§Ù„Ø¹Ù…Ø±: {age} Ø³Ù†Ø©
            - Ø§Ù„Ø¬Ù†Ø³: {"Ø£Ù†Ø«Ù‰" if gender == "female" else "Ø°ÙƒØ±"}
            - Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©: {"Ù…ØªØ²ÙˆØ¬" if marital_status == "married" else "Ø£Ø¹Ø²Ø¨"}
            - Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…: {education_level}

            **Ù†ØªØ§Ø¦Ø¬ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø´Ø®ØµÙŠØ© (Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø®Ù…Ø³ÙŠ Ø§Ù„ÙƒØ¨ÙŠØ±):**
            - Ø§Ù„Ø§Ù†ÙØªØ§Ø­ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø§Ø±Ø¨: {openness_pct:.1f}%
            - Ø§Ù„Ø¶Ù…ÙŠØ± ÙˆØ§Ù„Ø§Ù†Ø¶Ø¨Ø§Ø·: {conscientiousness_pct:.1f}%
            - Ø§Ù„Ø§Ù†Ø¨Ø³Ø§Ø·: {extraversion_pct:.1f}%
            - Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„ÙŠØ© ÙˆØ§Ù„ØªØ¹Ø§ÙˆÙ†: {agreeableness_pct:.1f}%
            - Ø§Ù„Ø¹ØµØ§Ø¨ÙŠØ©: {neuroticism_pct:.1f}%

            Ø§ÙƒØªØ¨ ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø· ÙŠØªØ¶Ù…Ù†:

            **ðŸŒŸ Ù†ÙˆØ¹ Ø§Ù„Ø´Ø®ØµÙŠØ©:**
            Ø­Ø¯Ø¯ Ù†ÙˆØ¹ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Ù…Ø«Ù„: Ø´Ø®ØµÙŠØ© Ù‚ÙŠØ§Ø¯ÙŠØ©ØŒ Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ©ØŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©ØŒ ØªØ­Ù„ÙŠÙ„ÙŠØ©ØŒ Ø¥Ù„Ø®)

            **ðŸ“Š Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙØµÙ„:**
            - ÙØ³Ø± ÙƒÙ„ Ø¨ÙØ¹Ø¯ Ù…Ù† Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø®Ù…Ø³Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ©
            - Ø§Ø±Ø¨Ø· Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø³Ù„ÙˆÙƒÙŠØ§Øª Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ©

            **ðŸ‘¤ Ø§Ù„ØªØ£Ø«ÙŠØ± Ø§Ù„Ø¯ÙŠÙ…ÙˆØºØ±Ø§ÙÙŠ:**
            ÙƒÙŠÙ ØªØ¤Ø«Ø± Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø®ØµÙŠØ©

            **ðŸ’ª Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ©:**
            Ø£Ù‡Ù… Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¹Ø§Ù„ÙŠØ©

            **ðŸŽ¯ Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ØªØ·ÙˆÙŠØ±:**
            Ø§Ù„Ù†ÙˆØ§Ø­ÙŠ Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†

            **ðŸ¢ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ù‡Ù†ÙŠØ©:**
            Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù‡Ù† Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø´Ø®ØµÙŠØ©

            **ðŸ“ Ù†ØµØ§Ø¦Ø­ Ø¹Ù…Ù„ÙŠØ©:**
            ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ù„Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ©

            **ðŸ¤ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©:**
            ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†

            **ðŸŽ¯ Ø§Ù„Ø®Ù„Ø§ØµØ©:**
            "Ø£Ù†Øª Ø´Ø®ØµÙŠØ© [Ø§Ø°ÙƒØ± Ù†ÙˆØ¹ Ø§Ù„Ø´Ø®ØµÙŠØ©]"

            Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù‡Ø§Ù…Ø©:
            - Ø§ÙƒØªØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ø¨Ø³ÙŠØ·Ø© ÙÙ‚Ø·
            - Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ ÙƒÙ„Ù…Ø§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            - Ø§Ø¬Ø¹Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø´Ø®ØµÙŠØ§Ù‹ ÙˆØ¹Ù…Ù„ÙŠØ§Ù‹
            - Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù…ÙˆØ² ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ù…Ù†Ø§Ø³Ø¨Ø©
            - Ø§Ø±Ø¨Ø· ÙƒÙ„ Ù†Ù‚Ø·Ø© Ø¨Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù„Ù„Ø´Ø®Øµ
            """

            # Call Groq API
            response = groq_client.chat.completions.create(
                model="llama3-8b-8192",
                messages=[
                    {"role": "system", "content": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù†ÙØ³ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø®ØµÙŠØ©. ØªÙƒØªØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·. ØªÙ‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙÙŠØ¯Ø© ÙˆØ¹Ù…Ù„ÙŠØ©. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ ÙƒÙ„Ù…Ø§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø£Ø¨Ø¯Ø§Ù‹."},
                    {"role": "user", "content": personality_prompt}
                ],
                temperature=0.7,
                max_tokens=2000
            )
            
            ai_analysis = response.choices[0].message.content
            print(f"âœ… AI Analysis generated successfully: {len(ai_analysis)} characters")
            return ai_analysis
            
        except Exception as e:
            print(f"âŒ Error using Groq AI: {e}")
            # Fall back to manual analysis
    
    # Manual fallback analysis if AI fails
    print(f"ðŸ”„ Using fallback analysis for {name}")
    
    # Determine personality type
    dominant_trait = max([
        ("Ù…Ù†ÙØªØ­ ÙˆÙ…Ø¨Ø¯Ø¹", openness_pct),
        ("Ù…Ù†Ø¶Ø¨Ø· ÙˆÙ…Ù†Ø¸Ù…", conscientiousness_pct), 
        ("Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙˆÙ†Ø´ÙŠØ·", extraversion_pct),
        ("Ù…ØªØ¹Ø§ÙˆÙ† ÙˆÙ„Ø·ÙŠÙ", agreeableness_pct),
        ("Ø­Ø³Ø§Ø³ ÙˆØ¹Ø§Ø·ÙÙŠ", neuroticism_pct)
    ], key=lambda x: x[1])
    
    analysis = f"""
ðŸŒŸ **ØªØ­Ù„ÙŠÙ„ Ø´Ø®ØµÙŠØ© {name} Ø§Ù„Ù…ÙØµÙ„**

**ðŸŽ¯ Ù†ÙˆØ¹ Ø§Ù„Ø´Ø®ØµÙŠØ©:**
Ø£Ù†Øª Ø´Ø®ØµÙŠØ© {dominant_trait[0]} - Ø­ÙŠØ« Ø£Ù† Ø£Ø¨Ø±Ø² ØµÙØ§ØªÙƒ Ù‡ÙŠ {dominant_trait[0]} Ø¨Ù†Ø³Ø¨Ø© {dominant_trait[1]:.1f}%

**ðŸ“Š Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙØµÙ„:**

**Ø§Ù„Ø§Ù†ÙØªØ§Ø­ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø§Ø±Ø¨ ({openness_pct:.1f}%):**
{'ðŸŽ¨ Ø£Ù†Øª Ø´Ø®Øµ Ù…Ø¨Ø¯Ø¹ ÙˆÙ…Ù†ÙØªØ­ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©ØŒ ØªØ­Ø¨ Ø§Ù„Ø§Ø³ØªØ·Ù„Ø§Ø¹ ÙˆØ§Ù„ØªØ¹Ù„Ù…' if openness_pct > 70 else 
 'ðŸ“š ØªÙØ¶Ù„ Ø§Ù„Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„Ù…Ø¬Ø±Ø¨Ø© ÙˆØ§Ù„Ù…Ø£Ù„ÙˆÙØ©ØŒ ÙˆÙ‡Ø°Ø§ ÙŠØ¬Ø¹Ù„Ùƒ Ù…ÙˆØ«ÙˆÙ‚Ø§Ù‹ ÙˆÙ…Ø³ØªÙ‚Ø±Ø§Ù‹' if openness_pct < 40 else
 'âš–ï¸ Ù…ØªÙˆØ§Ø²Ù† ÙÙŠ ØªÙ‚Ø¨Ù„ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±'}

**Ø§Ù„Ø¶Ù…ÙŠØ± ÙˆØ§Ù„Ø§Ù†Ø¶Ø¨Ø§Ø· ({conscientiousness_pct:.1f}%):**
{'ðŸ“‹ Ù…Ù†Ø¸Ù… Ø¬Ø¯Ø§Ù‹ ÙˆÙ…ÙˆØ«ÙˆÙ‚ØŒ ØªÙ„ØªØ²Ù… Ø¨Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ ÙˆØªÙƒÙ…Ù„ Ù…Ù‡Ø§Ù…Ùƒ Ø¨Ø¯Ù‚Ø©' if conscientiousness_pct > 70 else
 'ðŸŽˆ Ù…Ø±Ù† ÙˆØ¹ÙÙˆÙŠØŒ ØªØªÙƒÙŠÙ Ø¨Ø³Ù‡ÙˆÙ„Ø© Ù…Ø¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„Ù…ÙØ§Ø¬Ø¦Ø©' if conscientiousness_pct < 40 else
 'âš–ï¸ Ù…ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„ØªÙ†Ø¸ÙŠÙ… ÙˆØ§Ù„Ù…Ø±ÙˆÙ†Ø©ØŒ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ Ø§Ù„ØªØ®Ø·ÙŠØ· ÙˆØ§Ù„ØªÙƒÙŠÙ'}

**Ø§Ù„Ø§Ù†Ø¨Ø³Ø§Ø· ({extraversion_pct:.1f}%):**
{'ðŸŽ‰ Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ ÙˆÙ†Ø´ÙŠØ·ØŒ ØªØ³ØªÙ…Ø¯ Ø·Ø§Ù‚ØªÙƒ Ù…Ù† Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†' if extraversion_pct > 70 else
 'ðŸ¤” Ù‡Ø§Ø¯Ø¦ ÙˆÙ…ØªØ£Ù…Ù„ØŒ ØªÙØ¶Ù„ Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„ÙØ±Ø¯ÙŠØ© ÙˆØ§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ø¹Ù…ÙŠÙ‚' if extraversion_pct < 40 else
 'âš–ï¸ Ù…ØªÙˆØ§Ø²Ù† Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ§Ù‹ØŒ ØªØ³ØªØ·ÙŠØ¹ Ø§Ù„Ø§Ø³ØªÙ…ØªØ§Ø¹ Ø¨Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠØ© ÙˆØ§Ù„ÙØ±Ø¯ÙŠØ©'}

**Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„ÙŠØ© ({agreeableness_pct:.1f}%):**
{'ðŸ¤ Ù…ØªØ¹Ø§ÙˆÙ† ÙˆÙ…ØªØ³Ø§Ù…Ø­ØŒ ØªØ¶Ø¹ Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† ÙÙŠ Ø§Ø¹ØªØ¨Ø§Ø±Ùƒ' if agreeableness_pct > 70 else
 'ðŸ’ª Ù…Ø³ØªÙ‚Ù„ ÙˆØ­Ø§Ø²Ù…ØŒ ØªØ¯Ø§ÙØ¹ Ø¹Ù† Ø¢Ø±Ø§Ø¦Ùƒ ÙˆÙ„Ø§ ØªØªÙ†Ø§Ø²Ù„ Ø¨Ø³Ù‡ÙˆÙ„Ø©' if agreeableness_pct < 40 else
 'âš–ï¸ Ù…ØªÙˆØ§Ø²Ù† ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„ØŒ ØªØ³ØªØ·ÙŠØ¹ Ø§Ù„ØªØ¹Ø§ÙˆÙ† ÙˆØ§Ù„Ø­Ø²Ù… Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆÙ‚Ù'}

**Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ ({100-neuroticism_pct:.1f}%):**
{'ðŸ˜Œ Ù‡Ø§Ø¯Ø¦ ÙˆÙ…Ø³ØªÙ‚Ø± Ø¹Ø§Ø·ÙÙŠØ§Ù‹ØŒ ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¶ØºÙˆØ· Ø¨Ø«Ù‚Ø© ÙˆÙ‡Ø¯ÙˆØ¡' if neuroticism_pct < 30 else
 'ðŸ˜° Ø­Ø³Ø§Ø³ ÙˆÙ…ØªÙ‚Ù„Ø¨ Ø§Ù„Ù…Ø²Ø§Ø¬ØŒ ØªØªØ£Ø«Ø± Ø¨Ø§Ù„Ø¶ØºÙˆØ· ÙˆØ§Ù„ØªØºÙŠÙŠØ±Ø§Øª' if neuroticism_pct > 70 else
 'âš–ï¸ Ù…ØªÙˆØ§Ø²Ù† Ø¹Ø§Ø·ÙÙŠØ§Ù‹ØŒ ØªØ¸Ù‡Ø± Ù…Ø±ÙˆÙ†Ø© Ø¬ÙŠØ¯Ø© ÙÙŠ Ù…ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª'}

**ðŸ’¼ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ù‡Ù†ÙŠØ©:**
{
    "ðŸŽ“ Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙˆØ§Ù„ØªØ¯Ø±ÙŠØ¨" if extraversion_pct > 60 and agreeableness_pct > 60 else
    "ðŸ”¬ Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªØ·ÙˆÙŠØ±" if openness_pct > 70 and conscientiousness_pct > 60 else
    "ðŸ¥ Ø§Ù„Ø±Ø¹Ø§ÙŠØ© Ø§Ù„ØµØ­ÙŠØ©" if agreeableness_pct > 70 else
    "ðŸ’¼ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø¹Ù…Ø§Ù„" if conscientiousness_pct > 70 and extraversion_pct > 50 else
    "ðŸŽ¨ Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ©" if openness_pct > 70 else
    "ðŸ“Š Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª" if conscientiousness_pct > 60 else
    "ðŸ¤ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©"
}

**ðŸ“ Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ©:**
â€¢ Ø§Ø³ØªØ«Ù…Ø± Ù†Ù‚Ø§Ø· Ù‚ÙˆØªÙƒ ÙÙŠ ØªØ·ÙˆÙŠØ± Ù…Ù‡Ø§Ø±Ø§ØªÙƒ Ø§Ù„Ù…Ù‡Ù†ÙŠØ©
â€¢ ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø£Ø´Ø®Ø§Øµ ÙŠÙƒÙ…Ù„ÙˆÙ† Ø´Ø®ØµÙŠØªÙƒ ÙˆÙŠØ¯Ø¹Ù…ÙˆÙ† Ù†Ù…ÙˆÙƒ
â€¢ {"Ø§Ø¹Ø· ÙˆÙ‚ØªØ§Ù‹ Ø£ÙƒØ«Ø± Ù„Ù„Ø§Ø³ØªØ±Ø®Ø§Ø¡ ÙˆØ¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¶ØºÙˆØ·" if neuroticism_pct > 60 else "Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ø³ØªÙ‚Ø±Ø§Ø±Ùƒ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ Ø§Ù„Ù…Ù…ØªØ§Ø²"}
â€¢ {"Ø§Ø³ØªØºÙ„ Ø­Ø¨Ùƒ Ù„Ù„Ø§Ø³ØªØ·Ù„Ø§Ø¹ ÙÙŠ ØªØ¹Ù„Ù… Ù…Ù‡Ø§Ø±Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©" if openness_pct > 60 else "Ø§Ø¹Ø· Ù†ÙØ³Ùƒ ÙØ±ØµØ© Ù„ØªØ¬Ø±Ø¨Ø© Ø£Ø´ÙŠØ§Ø¡ Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø´ÙƒÙ„ ØªØ¯Ø±ÙŠØ¬ÙŠ"}

**ðŸŽ¯ Ø§Ù„Ø®Ù„Ø§ØµØ©:**
Ø£Ù†Øª Ø´Ø®ØµÙŠØ© {dominant_trait[0]} ØªØªÙ…ÙŠØ² Ø¨Ù‚Ø¯Ø±Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø© ÙˆØ¥Ù…ÙƒØ§Ù†ÙŠØ§Øª ÙƒØ¨ÙŠØ±Ø© Ù„Ù„Ù†Ù…Ùˆ ÙˆØ§Ù„ØªØ·ÙˆØ±.
"""
    
    return analysis

@app.get("/api/sessions/{session_id}/report")
async def get_report(session_id: str):
    try:
        if session_id not in sessions:
            raise HTTPException(status_code=404, detail="Session not found")
        
        session = sessions[session_id]
        
        if session["status"] != "completed":
            raise HTTPException(status_code=400, detail="Test not completed yet")
        
        # Calculate real scores from session data
        def calculate_dimension_score(dimension):
            """Calculate actual score from answered questions"""
            answered_questions = session["questions_answered"].get(dimension, [])
            if not answered_questions:
                return 50  # Default middle score
            
            total_score = 0
            for q in answered_questions:
                score = q["response"]
                # Handle reverse scored questions (assumed to be marked in question data)
                # For now, we'll use the raw score
                total_score += score
            
            # Convert to 0-100 scale (5 point scale * max questions per dimension)
            max_possible = len(answered_questions) * 5
            percentage = (total_score / max_possible) * 100 if max_possible > 0 else 50
            return min(100, max(0, percentage))
        
        def get_level(score):
            if score >= 75:
                return "Ø¹Ø§Ù„ÙŠ"
            elif score >= 50:
                return "Ù…ØªÙˆØ³Ø·"
            else:
                return "Ù…Ù†Ø®ÙØ¶"
        
        # Calculate real scores
        openness_score = calculate_dimension_score("openness")
        conscientiousness_score = calculate_dimension_score("conscientiousness")
        extraversion_score = calculate_dimension_score("extraversion")
        agreeableness_score = calculate_dimension_score("agreeableness")
        neuroticism_score = calculate_dimension_score("neuroticism")
        
        return {
            "session_id": session_id,
            "name": session["name"],
            "completion_date": "2025-01-24T10:30:00Z",
            "scores": {
                "openness": {
                    "name": "Ø§Ù„Ø§Ù†ÙØªØ§Ø­ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø§Ø±Ø¨",
                    "score": (openness_score / 100) * 4 + 1,  # Convert from 0-100% to 1-5 scale
                    "level": get_level(openness_score)
                },
                "conscientiousness": {
                    "name": "Ø§Ù„Ø¶Ù…ÙŠØ± Ø§Ù„Ø­ÙŠ",
                    "score": (conscientiousness_score / 100) * 4 + 1,
                    "level": get_level(conscientiousness_score)
                },
                "extraversion": {
                    "name": "Ø§Ù„Ø§Ù†Ø¨Ø³Ø§Ø·",
                    "score": (extraversion_score / 100) * 4 + 1,
                    "level": get_level(extraversion_score)
                },
                "agreeableness": {
                    "name": "Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„ÙŠØ©",
                    "score": (agreeableness_score / 100) * 4 + 1,
                    "level": get_level(agreeableness_score)
                },
                "neuroticism": {
                    "name": "Ø§Ù„Ø¹ØµØ§Ø¨ÙŠØ©",
                    "score": (neuroticism_score / 100) * 4 + 1,
                    "level": get_level(neuroticism_score)
                }
            },
            "detailed_analysis": generate_comprehensive_analysis(
                {
                    "openness": (openness_score / 100) * 4 + 1,
                    "conscientiousness": (conscientiousness_score / 100) * 4 + 1,
                    "extraversion": (extraversion_score / 100) * 4 + 1,
                    "agreeableness": (agreeableness_score / 100) * 4 + 1,
                    "neuroticism": (neuroticism_score / 100) * 4 + 1
                },
                session
            ),
            "recommendations": [
                "Ø§Ø³ØªÙ…Ø± ÙÙŠ ØªØ·ÙˆÙŠØ± Ù†Ù‚Ø§Ø· Ù‚ÙˆØªÙƒ",
                "Ø§Ø¹Ù…Ù„ Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ Ù„ØªØ·ÙˆÙŠØ±",
                "ØªØ°ÙƒØ± Ø£Ù† Ø§Ù„Ø´Ø®ØµÙŠØ© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù†Ù…Ùˆ ÙˆØ§Ù„ØªØ·ÙˆÙŠØ±"
            ]
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error generating report: {str(e)}")

@app.get("/")
async def root():
    return {"message": "Personality Test API is running"}

@app.get("/api/test")
async def test_connection():
    return {"status": "success", "message": "API connection is working"}

# Admin endpoints
@app.post("/api/admin/login")
async def admin_login(request: AdminLoginRequest):
    """Admin login endpoint"""
    try:
        # Simple hardcoded admin credentials
        if request.username == "admin" and request.password == "PersonalityAdmin2025!":
            # Generate a simple token (in production, use proper JWT)
            token = "admin_token_" + str(uuid.uuid4())
            return AdminLoginResponse(
                success=True,
                token=token,
                message="ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¨Ù†Ø¬Ø§Ø­"
            )
        else:
            return AdminLoginResponse(
                success=False,
                message="Ø¨ÙŠØ§Ù†Ø§Øª ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ ØºÙŠØ± ØµØ­ÙŠØ­Ø©"
            )
    except Exception as e:
        return AdminLoginResponse(
            success=False,
            message="Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø³ÙŠØ±ÙØ±"
        )

@app.get("/api/admin/test")
async def admin_test():
    """Test admin connection"""
    try:
        load_sessions()
        total_sessions = len(sessions)
        total_answers = sum(len(session.get('questions_answered', {}).get(dim, [])) 
                          for session in sessions.values() 
                          for dim in ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'])
        
        return {
            "status": "success",
            "sessionsCount": total_sessions,
            "answersCount": total_answers,
            "message": "Ø§ØªØµØ§Ù„ Admin Ù†Ø¬Ø­"
        }
    except Exception as e:
        return {
            "status": "error", 
            "message": f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„: {str(e)}"
        }

@app.get("/api/admin/dashboard")
async def get_admin_dashboard():
    """Get dashboard statistics"""
    try:
        # Always load fresh data from file
        load_sessions()
        
        print(f"DEBUG: Dashboard called - Total sessions in memory: {len(sessions)}")
        print(f"DEBUG: Sessions dictionary reference: {id(sessions)}")
        print(f"DEBUG: Sessions keys: {list(sessions.keys())}")
        
        if len(sessions) > 0:
            print(f"DEBUG: First session sample: {list(sessions.values())[0]}")
        
        total_sessions = len(sessions)
        completed_sessions = len([s for s in sessions.values() if s["status"] == "completed"])
        active_sessions = len([s for s in sessions.values() if s["status"] == "active"])
        
        print(f"DEBUG: Calculated stats - Total: {total_sessions}, Completed: {completed_sessions}, Active: {active_sessions}")
        
        # Gender distribution - convert to array format expected by frontend
        genders = [s.get("gender", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯") for s in sessions.values() if s.get("gender")]
        gender_counts = {}
        for gender in genders:
            gender_counts[gender] = gender_counts.get(gender, 0) + 1
        
        # Convert gender distribution to array format
        gender_distribution = []
        for gender, count in gender_counts.items():
            gender_distribution.append({
                "label": "Ø°ÙƒØ±" if gender == "male" else "Ø£Ù†Ø«Ù‰" if gender == "female" else gender,
                "value": count
            })
        
        # Education level distribution - convert to array format expected by frontend
        education_levels = [s.get("education_level", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯") for s in sessions.values() if s.get("education_level")]
        education_counts = {}
        for edu in education_levels:
            education_counts[edu] = education_counts.get(edu, 0) + 1
        
        # Convert education distribution to array format
        education_distribution = []
        for edu, count in education_counts.items():
            education_distribution.append({
                "label": edu,
                "value": count
            })
        
        # Age distribution - calculate ages and group them
        ages = []
        current_year = datetime.now().year
        for s in sessions.values():
            if s.get("birth_year"):
                age = current_year - s.get("birth_year")
                ages.append(age)
        
        age_groups = {"18-25": 0, "26-35": 0, "36-45": 0, "46-55": 0, "56+": 0}
        for age in ages:
            if 18 <= age <= 25:
                age_groups["18-25"] += 1
            elif 26 <= age <= 35:
                age_groups["26-35"] += 1
            elif 36 <= age <= 45:
                age_groups["36-45"] += 1
            elif 46 <= age <= 55:
                age_groups["46-55"] += 1
            elif age > 55:
                age_groups["56+"] += 1
        
        # Convert age distribution to array format
        age_distribution = []
        for age_group, count in age_groups.items():
            if count > 0:  # Only include groups that have participants
                age_distribution.append({
                    "label": age_group,
                    "value": count
                })
        
        # Mock daily stats for the chart
        daily_stats = [
            {"date": "2025-08-01", "newParticipants": 5, "completedTests": 3},
            {"date": "2025-08-02", "newParticipants": 8, "completedTests": 6},
            {"date": "2025-08-03", "newParticipants": total_sessions, "completedTests": completed_sessions},
        ]
        
        return {
            "totalParticipants": total_sessions,
            "completedTests": completed_sessions,
            "activeSessions": active_sessions,
            "completionRate": (completed_sessions / total_sessions * 100) if total_sessions > 0 else 0,
            "genderDistribution": gender_distribution,
            "educationDistribution": education_distribution,
            "ageDistribution": age_distribution,
            "dailyStats": daily_stats,
            "averageCompletionRate": (completed_sessions / total_sessions * 100) if total_sessions > 0 else 0
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting dashboard data: {str(e)}")

@app.get("/api/admin/participants")
async def get_participants(page: int = 1, search: str = ""):
    """Get participants list with pagination"""
    try:
        # Always load fresh data from file
        load_sessions()
        
        participants_list = []
        for session_id, session in sessions.items():
            if search and search.lower() not in session.get("name", "").lower():
                continue
                
            # Format dates properly and calculate age
            completion_date = None
            if session.get("status") == "completed" and session.get("completed_at"):
                completion_date = session.get("completed_at")[:10]  # Extract date part only (YYYY-MM-DD)
            
            # Calculate age from birth year
            age = None
            if session.get("birth_year"):
                current_year = datetime.now().year
                age = current_year - session.get("birth_year")
            
            participant = {
                "sessionId": session_id,
                "name": session.get("name", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"),
                "gender": "Ø°ÙƒØ±" if session.get("gender") == "male" else "Ø£Ù†Ø«Ù‰" if session.get("gender") == "female" else "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                "birthYear": session.get("birth_year", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"),
                "age": age if age else "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                "educationLevel": session.get("education_level", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"),
                "maritalStatus": session.get("marital_status", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"),
                "status": "Ù…ÙƒØªÙ…Ù„" if session.get("status") == "completed" else "Ù†Ø´Ø·" if session.get("status") == "active" else "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                "currentDimension": session.get("current_dimension", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"),
                "questionsAnswered": sum(session.get("dimension_question_count", {}).values()),
                "completionDate": completion_date
            }
            participants_list.append(participant)
        
        # Simple pagination
        per_page = 10
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        paginated_participants = participants_list[start_idx:end_idx]
        
        total_pages = (len(participants_list) + per_page - 1) // per_page
        
        return {
            "participants": paginated_participants,
            "totalPages": total_pages,
            "currentPage": page,
            "totalParticipants": len(participants_list)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting participants: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    import os
    
    # Add sample data on startup
    add_sample_data()
    
    # Use environment PORT for Hugging Face, fallback to 8889 for local
    port = int(os.getenv("PORT", 8889))
    uvicorn.run(app, host="0.0.0.0", port=port)
